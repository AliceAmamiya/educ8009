[
  {
    "objectID": "session11-12/assignment.html",
    "href": "session11-12/assignment.html",
    "title": "Laboratory 06",
    "section": "",
    "text": "I just realized how sh*t I actually knew about regression before (as Figure 1 shows). I’ll be revising my work next week since I got caught up with multiple deadlines from my advisor1.\nI’m dying.\n\n\n\n\n\n\nFigure 1: I think I was here.\n\n\n\n\n\nI’ve open-sourced all the labs, including questions and solutions, on my GitHub repository. Unlike PDFs, the web version can be updated at any time — even after it’s published. The flexibility of HTML may reflects the evolving way we produce knowledge in academia.",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#about-the-web-doc",
    "href": "session11-12/assignment.html#about-the-web-doc",
    "title": "Laboratory 06",
    "section": "",
    "text": "I’ve open-sourced all the labs, including questions and solutions, on my GitHub repository. Unlike PDFs, the web version can be updated at any time — even after it’s published. The flexibility of HTML may reflects the evolving way we produce knowledge in academia.",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#talk-is-cheap.-lets-face-the-fear.",
    "href": "session11-12/assignment.html#talk-is-cheap.-lets-face-the-fear.",
    "title": "Laboratory 06",
    "section": "Talk is cheap. Let’s face the fear.",
    "text": "Talk is cheap. Let’s face the fear.\n\nimport pandas as pd\nimport pyreadstat\n\nfear_df = pd.read_spss('./datasets/fear.sav', convert_categoricals=False)\nfear_questions = pyreadstat.read_sav('./datasets/fear.sav')[1].column_labels\n\n\n# List all questions asked. \ndef list_questions(): \n    n=1\n    for question in fear_questions: \n        print(f'Q{n}: {question}')\n        n += 1\n\nlist_questions()\n\nQ1: Statistics makes me cry\nQ2: Standard deviations excite me\nQ3: I dream that Pearson is attacking me with correlation coefficients\nQ4: I don't understand statistics\nQ5: People try to tell you that SPSS makes statistics easier to understand but it doesn't\nQ6: I weep openly at the mention of central tendency\nQ7: I can't sleep for thoughts of effect sizes\nQ8: I wake up under my duvet thinking that I am trapped under a normal distribution\n\n\n\n\n# Shape of the 'fear'\nprint(f'Rows vs Columns: ', fear_df.shape)\n\n# Columns include in this dataset\nprint(f'Name of columns: ', fear_df.columns)\n\n# Describe it!\nprint(f'Description: \\n', fear_df.describe())\n\nRows vs Columns:  (2571, 8)\nName of columns:  Index(['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8'], dtype='object')\nDescription: \n                 Q1           Q2           Q3           Q4           Q5  \\\ncount  2571.000000  2571.000000  2571.000000  2571.000000  2571.000000   \nmean      3.483469     3.481136     3.471023     3.465189     3.494360   \nstd       0.986297     0.990585     0.994262     1.000707     0.981674   \nmin       1.000000     1.000000     1.000000     1.000000     1.000000   \n25%       3.000000     3.000000     3.000000     3.000000     3.000000   \n50%       4.000000     4.000000     3.000000     3.000000     4.000000   \n75%       4.000000     4.000000     4.000000     4.000000     4.000000   \nmax       5.000000     5.000000     5.000000     5.000000     5.000000   \n\n                Q6           Q7           Q8  \ncount  2571.000000  2571.000000  2571.000000  \nmean      3.473357     3.498639     3.475690  \nstd       0.982913     0.998588     0.978212  \nmin       1.000000     1.000000     1.000000  \n25%       3.000000     3.000000     3.000000  \n50%       3.000000     4.000000     4.000000  \n75%       4.000000     4.000000     4.000000  \nmax       5.000000     5.000000     5.000000  \n\n\nWait, \\(N = 2571\\)? The entire FED doesn’t even have this many students!\nI then plotted the Likert-scale (see Figure 2) to see the distribution of everyone’s fear:\n\n# Better to visualize the data: \n\nimport matplotlib.pyplot as plt\nimport plot_likert\n\nfear_scales = range(1,6)\nfear_scales_labels = ['Strongly Disagree', 'Disagree', 'Neither', 'Agree', 'Strongly agree']\nfear_plot = plot_likert.plot_likert(fear_df, fear_scales, plot_percentage=True)\nhandles, labels = fear_plot.get_legend_handles_labels()\nfear_plot.legend(handles, fear_scales_labels, bbox_to_anchor=(1.0, 1.0))\nplt.show()\n\n/home/rshen/miniconda3/envs/educ8009/lib/python3.10/site-packages/plot_likert/plot_likert.py:257: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df.applymap(validate)\n/home/rshen/miniconda3/envs/educ8009/lib/python3.10/site-packages/plot_likert/plot_likert.py:310: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  responses_to_first_question = responses_per_question[0]\n\n\n\n\n\n\n\n\nFigure 2: A visualized Likert-scale result on all questions.",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#q1-cronbachs-alpha-for-all-items-except-item-2",
    "href": "session11-12/assignment.html#q1-cronbachs-alpha-for-all-items-except-item-2",
    "title": "Laboratory 06",
    "section": "Q1: Cronbach’s \\(\\alpha\\) for all items (except item 2)",
    "text": "Q1: Cronbach’s \\(\\alpha\\) for all items (except item 2)\nAnswer\nFor \\(Q_1 + Q_3 + ... + Q_7 + Q_8\\), the \\(\\alpha \\approx 0.6873\\).\nSolution\n\nimport pingouin as pg\n\nfear_df_item2_excluded = fear_df.drop('Q2', axis=1)\n\n# Calculate cronbach's alpha within a dataframe: \ndef print_cronbach_alpha(df): \n    alpha = pg.cronbach_alpha(data=df)[0]\n    print(f'Cronbach\\'s Alpha:',alpha)\n\nprint_cronbach_alpha(fear_df_item2_excluded)\nprint('for the original Q2 deleted.')\n\nCronbach's Alpha: 0.6873382583349454\nfor the original Q2 deleted.",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#q2-drop-item-to-improve-the-scale",
    "href": "session11-12/assignment.html#q2-drop-item-to-improve-the-scale",
    "title": "Laboratory 06",
    "section": "Q2: Drop item to improve the scale",
    "text": "Q2: Drop item to improve the scale\nAnswer\nI would like to drop Question 5, “People try to tell you that SPSS makes statistics easier to understand but it doesn’t”, for following reason:\n\nThe lowest inter-item correlation: see Table 1, which was generated from SPSS (sorry about that).\nThe lowest corrected item-total correlation: \\(0.2593\\)\nCronbach’s alpha increases when this item is deleted: from \\(\\alpha \\approx 0.6873\\) to \\(\\alpha \\approx 0.6888\\).2\n\n\n\n\nTable 1: Inter-Item Correlation Matrix\n\n\n\n\n\n\nQ1\nQ3\nQ4\nQ5\nQ6\nQ7\nQ8\n\n\n\n\nQ1\n1.000\n.217\n.223\n.150\n.176\n.207\n.227\n\n\nQ3\n.217\n1.000\n.319\n.196\n.271\n.284\n.320\n\n\nQ4\n.223\n.319\n1.000\n.187\n.240\n.341\n.371\n\n\nQ5\n.150\n.196\n.187\n1.000\n.153\n.123\n.164\n\n\nQ6\n.176\n.271\n.240\n.153\n1.000\n.232\n.264\n\n\nQ7\n.207\n.284\n.341\n.123\n.232\n1.000\n.349\n\n\nQ8\n.227\n.320\n.371\n.164\n.264\n.349\n1.000\n\n\n\n\n\n\nSolution\n\n# Calculate item-total correlations\ndef item_total_correlation(df):\n    total_score = df.sum(axis=1)\n    correlations = {}\n    for question in df.columns:\n        total_excluding_item = total_score - df[question]\n        correlation = df[question].corr(total_excluding_item)\n        correlations[question] = correlation\n    return correlations\n\n# Calculate the Cronbach's alpha after deleting each item\ndef cronbach_alpha_if_deleted(df):\n    alphas = {}\n    for question in df.columns:\n        df_without_item = df.drop(question, axis=1)\n        alpha = pg.cronbach_alpha(df_without_item)[0]\n        alphas[question] = alpha\n    return alphas\n\n# Calculate item-total correlation\nitem_correlations = item_total_correlation(fear_df_item2_excluded)\nprint(f'Item-total Correlations:')\nfor item in item_correlations: \n    print(item, 'to total', item_correlations[item])\nprint(f'======================')\nalphas_if_deleted = cronbach_alpha_if_deleted(fear_df_item2_excluded)\nprint(f'Cronbach\\'s Alpha if an item deleted:')\nfor item in alphas_if_deleted: \n    print(f'If {item} is deleted: ', alphas_if_deleted[item])\n\n# Present the result\nprint(f'======================')\nbest_item_to_delete_corr = min(item_correlations, key=item_correlations.get)\nprint(f'Item that has lowest item-total correlation coefficient: ', best_item_to_delete_corr)\nbest_item_to_delete_alpha = max(alphas_if_deleted, key=alphas_if_deleted.get)\nprint(f'Item that would improve Cronbach\\'s alpha the most if deleted: {best_item_to_delete_alpha}')\n\nItem-total Correlations:\nQ1 to total 0.32493144386994777\nQ3 to total 0.44890601739301994\nQ4 to total 0.4726060003637261\nQ5 to total 0.2592530237967403\nQ6 to total 0.36566649644104937\nQ7 to total 0.4267725068168662\nQ8 to total 0.47718130451492624\n======================\nCronbach's Alpha if an item deleted:\nIf Q1 is deleted:  0.672004161791457\nIf Q3 is deleted:  0.6384165554243204\nIf Q4 is deleted:  0.6315956086496489\nIf Q5 is deleted:  0.6887820469122623\nIf Q6 is deleted:  0.6611984636617284\nIf Q7 is deleted:  0.6445525162609321\nIf Q8 is deleted:  0.6308404954800185\n======================\nItem that has lowest item-total correlation coefficient:  Q5\nItem that would improve Cronbach's alpha the most if deleted: Q5",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#q3-recode-then-describe-the-distribution-of-q2",
    "href": "session11-12/assignment.html#q3-recode-then-describe-the-distribution-of-q2",
    "title": "Laboratory 06",
    "section": "Q3: Recode, then describe the distribution of Q2",
    "text": "Q3: Recode, then describe the distribution of Q2\nAnswer\nThe distribution of the reverse-worded Question 2, after recoding, shows a mean score of \\(M = 2.52\\), suggesting moderate agreement with standard deviations excites the participants. The standard deviation is \\(SD = 0.99\\), indicates moderate variability among responses. The distribution, as shown in Figure 3, is slightly right-skewed, with a peak at \\(2\\) (that is Disagree), indicating that a majority of students (also see in Figure 4) reported that they disagree on the statement that standard deviations excites them.\nSolution\n\n# Recoding the reverse-worded item, this part works the same as SPSS: \nfear_df_rev = fear_df\nfear_df_rev['Q2'] = 6 - fear_df_rev['Q2']\n\n\n# Describe the \nfear_df_rev['Q2'].describe()\n\ncount    2571.000000\nmean        2.518864\nstd         0.990585\nmin         1.000000\n25%         2.000000\n50%         2.000000\n75%         3.000000\nmax         5.000000\nName: Q2, dtype: float64\n\n\n\nimport seaborn as sns\n\nq2_rev_hist = sns.histplot(fear_df_rev['Q2'], bins=5)\nq2_rev_hist.set_xticks(fear_scales)\nq2_rev_hist.set_title(fear_questions[1])\nplt.show()\n\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\nq2_rev_plot = plot_likert.plot_likert(fear_df_rev['Q2'], fear_scales, plot_percentage=True)\nq2_rev_plot.legend(handles, fear_scales_labels, bbox_to_anchor=(1.0, 1.0))\nq2_rev_plot.set_title(fear_questions[1])\nplt.show()\n\n/home/rshen/miniconda3/envs/educ8009/lib/python3.10/site-packages/plot_likert/plot_likert.py:257: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df.applymap(validate)\n/home/rshen/miniconda3/envs/educ8009/lib/python3.10/site-packages/plot_likert/plot_likert.py:310: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  responses_to_first_question = responses_per_question[0]\n\n\n\n\n\n\n\n\nFigure 4: The result of Question 2 (recoded)",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#q4-cronbachs-alpha-of-all-the-items",
    "href": "session11-12/assignment.html#q4-cronbachs-alpha-of-all-the-items",
    "title": "Laboratory 06",
    "section": "Q4: Cronbach’s alpha of all the items",
    "text": "Q4: Cronbach’s alpha of all the items\nAnswer\nFor \\(Q_1 + Q_{2rev} + ... + Q_7 + Q_8\\), the Cronbach’s \\(\\alpha \\approx 0.7071\\)\nSolution\n\nprint_cronbach_alpha(fear_df_rev)\nprint('with recoded Q2_rev')\n\nCronbach's Alpha: 0.7071483372463728\nwith recoded Q2_rev",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "session11-12/assignment.html#footnotes",
    "href": "session11-12/assignment.html#footnotes",
    "title": "Laboratory 06",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSo did you finally have lunch with him?.↩︎\nWhat a fortune number for we Asian people.↩︎",
    "crumbs": [
      "Labs",
      "Session11 12",
      "Laboratory 06"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lecture Notes / Assignments\non EDUC8009",
    "section": "",
    "text": "ようこそ!\nThis is my personal repository for datasets, lecture notes & assignment submissions on EDUC8009 - DESCRIPTIVE AND INFERENTIAL STATISTICS.\nAs this course is tailored for SPSS users, here’s the problem: I can only get my hands on the legal copy in our computer lab, where the machines are about as speedy as a sloth on a lazy day, sporting a single-channel 8GB of RAM. Apple may claim that 8 is enough (even greater than 16) for most users, and we all know that’s just some optimistic marketing.\nSo, I decided to take a detour and embrace the open-source revolution with sweety sidekick: python=3.10 armed with the duo of Doolin and Dal…, sorry, pandas and scipy.stats. An R=4.3.3 implementation will also be included in the solution of my assignment as well.\n\n\n\nIt all starts from iris and mtcars.\n\n\nRiko",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "session03/assignment.html",
    "href": "session03/assignment.html",
    "title": "Laboratory 01",
    "section": "",
    "text": "A biologist was worried about the potential health effects of music festivals. So, one year she went to the Download Music Festival (http://www.downloadfestival.co.uk) and measured the hygiene of 810 concert goers over the three days of the festival. In theory each person was measured on each day but because it was difficult to track people down, there were some missing data on days 2 and 3. Hygiene was measured using a technique that results in a score ranging between 0 (you smell like a rotting corpse) and 5 (you smell like sweet roses). Sanitation is not always great at these places, so this researcher predicted that personal hygiene would go down dramatically over the three days of the festival. The data file is called download.sav.",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session03/assignment.html#preparation",
    "href": "session03/assignment.html#preparation",
    "title": "Laboratory 01",
    "section": "Preparation",
    "text": "Preparation\nBefore working on the lab work , I started with a quick EDA.\n\ndf_download, metadata_download = pyreadstat.read_sav(\"./datasets/download.sav\")\n\n# A quick peek on the structure of the DataFrame. \nprint('0. An overall description of this dataset: \\n' + str(df_download.describe()) + '\\n')\nprint('1. Shape - rows and columns: \\n' + str(df_download.shape) + '\\n') \nprint('2. Variables (in SPSS) or column names: \\n ' + str(df_download.columns) + '\\n')\nprint('3. Missing values (if any): \\n ' + str(df_download.isnull().sum()) + '\\n')\n\n0. An overall description of this dataset: \n         ticket_no      gender       day_1       day_2       day_3\ncount   810.000000  810.000000  810.000000  264.000000  123.000000\nmean   3616.212346    1.769136    1.793358    0.960909    0.976504\nstd     610.241493    0.632679    0.944495    0.720780    0.710277\nmin    2111.000000    1.000000    0.020000    0.000000    0.020000\n25%    3096.250000    1.000000    1.312500    0.410000    0.440000\n50%    3620.500000    2.000000    1.790000    0.790000    0.760000\n75%    4154.750000    2.000000    2.230000    1.350000    1.525000\nmax    4765.000000    3.000000   20.020000    3.440000    3.410000\n\n1. Shape - rows and columns: \n(810, 5)\n\n2. Variables (in SPSS) or column names: \n Index(['ticket_no', 'gender', 'day_1', 'day_2', 'day_3'], dtype='object')\n\n3. Missing values (if any): \n ticket_no      0\ngender         0\nday_1          0\nday_2        546\nday_3        687\ndtype: int64\n\n\n\nFrom the output, it seems something going wrong with the column day_1. I’m sure I’ll check it later. Besides, the column gender should be a categorical variable rather than an int and should be corrected:\n\ndf_download['gender'] = df_download['gender'].astype('category')",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session03/assignment.html#question-1",
    "href": "session03/assignment.html#question-1",
    "title": "Laboratory 01",
    "section": "Question 1",
    "text": "Question 1\n\nQ1a: What do 1, 2, and 3 mean in variable “gender”?\n\nAnswer:\n\n\n\nCode\nLabel\n\n\n\n\n1\nMale\n\n\n2\nFemale\n\n\n3\nNon-Binary\n\n\n\nSolution:\nI reached the mapping for categorical variables by accessing metadata from the given sav file by pyreadstats:\n\nprint(metadata_download.variable_value_labels)\n\n{'gender': {1.0: 'Male', 2.0: 'Female', 3.0: 'Non-binary'}}\n\n\nP.S. On another note, considering this is data from a music festival, should I assume that apart from the 90 individuals labeled themselves as ENBY, the terms “Female” and “Male” in the dataset include both cisgender and transgender individuals?\n\nQ1b: What are their percentages in the sample?\n\nAnswer:\n\n\n\nGender\nCount\nPercentage\n\n\n\n\nFemale\n443\n54.69%\n\n\nMale\n277\n34.20%\n\n\nNon-Binary\n90\n11.11%\n\n\n\nSolution:\n\n# Calculating the Percentage\npercentage = df_download['gender'].value_counts(normalize=True) * 100   # For percentage\nprint(percentage)\n\ngender\n2.0    54.691358\n1.0    34.197531\n3.0    11.111111\nName: proportion, dtype: float64\n\n\n\nReport the result (in a formal way)\nAnd the results of Question 1a and 1b should be reported in a formal way:\nThe total \\(N\\) for ths dataset (for the study) was 810. The sample included the following gender groups: 54.69% male, 34.20% female and 11.11% of non-binary.",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session03/assignment.html#question-2",
    "href": "session03/assignment.html#question-2",
    "title": "Laboratory 01",
    "section": "Question 2",
    "text": "Question 2\n\nDraw a barplot of “gender” whose 𝑦-axis represents the percentage of each group.\n\nAnswer:\nSee Figure 1\nSolution:\n\ngender_barplot = sns.barplot(percentage)\ngender_barplot.set_xticklabels(['Male', 'Female', 'Non-binary'])\nplt.show()\n\nC:\\Users\\Riko\\AppData\\Local\\Temp\\ipykernel_15292\\2800231647.py:4: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  gender_barplot.set_xticklabels(['Male', 'Female', 'Non-binary'])\n\n\n\n\n\n\n\n\nFigure 1: Count of Gender",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session03/assignment.html#question-3",
    "href": "session03/assignment.html#question-3",
    "title": "Laboratory 01",
    "section": "Question 3",
    "text": "Question 3\n\nDraw a histogram of day_1. Is there anything wrong with this variable?\n\nAnswer:\nSee Figure 2.\nSolution:\nFrom the EDA and the histogram shown below (see Figure 2), we can clearly identify an outlier with a “hygiene score” significantly higher than the others. Given that the score should range between 0 and 5, this could probability be a typo or some other thing we may have overlooked.\n\nday1_hist = sns.histplot(df_download['day_1'])\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Distribution of Hygiene Score in Day 1\n\n\n\n\n\n\n# Filter out the outlier\nprint(df_download[df_download['day_1'] &gt; 5.0])\n\n     ticket_no gender  day_1  day_2  day_3\n610     4158.0    2.0  20.02   2.44    NaN\n\n\nI filtered out this outlier, it’s in row number 610, and it turns out that based on the ranking, this lady was four times cleaner than the cleanest person at the music festival. Hmm… 4x cleaner? Sounds like something you’d read on a box of laundry pods (see Figure 3). Let’s call her Ms. Laundry Pod.\n\n\n\n\n\n\nFigure 3: 4x-cleaning-power\n\n\n\nI’m just saying this for fun (except the Ms. Laundry Pod part). The hygiene score is an interval variable that we can’t simply multiply or divide the values like what I did.",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session03/assignment.html#question-4",
    "href": "session03/assignment.html#question-4",
    "title": "Laboratory 01",
    "section": "Question 4",
    "text": "Question 4\n\nHow many cases have missing values for day_2?\n\nAnswer:\n546 cases have missing values in the column day_2.\nSolution:\n\nprint(df_download.isnull().sum()['day_2'])\n\n546",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session03/assignment.html#question-5",
    "href": "session03/assignment.html#question-5",
    "title": "Laboratory 01",
    "section": "Question 5",
    "text": "Question 5\n\nExcluding the outlier in day 1.\n\nSolution:\nThankfully, there’s only one outlier - Ms. Laundry Pod, located at row number 610. So, for this part, I’ll simply exclude that row and save the remaining data to a new DataFrame.\n\ndf_download_dropped_outlier = df_download.drop(610)\n# Or I could just enter the danger zone by dropping data on the DataFrame by:  \n# df_download.drop(610, inplace=True)\n\nAlso, I double checked by plotting the data once more (see Figure 4).\n\n# A plot to confirm if I'm on the right track. \ndf_download_dropped_outlier['day_1'].hist()\n\n\n\n\n\n\n\nFigure 4: Distribution of Hygiene Score in Day 1\n\n\n\n\n\n[EOF]",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01"
    ]
  },
  {
    "objectID": "session15-16/assignment.html",
    "href": "session15-16/assignment.html",
    "title": "Laboratory 08",
    "section": "",
    "text": "Mobile phones emit microwaves, and so holding one next to your brain for large parts of the day is a bit like sticking your brain in a microwave oven and pushing the “cook until well done” button. If we wanted to test this experimentally, we could get six groups of people and strap a mobile phone on their heads, then by remote control turn the phones on for a certain amount of time each day 1. After six months 2, we measure the size of any tumour3 (in mm3) close to the site of the phone antenna (just behind the ear). The six groups experienced 0, 1, 2, 3, 4, or 5 hours per day of phone microwaves for 6 months 4. Do tumours significantly increase with greater daily exposure? The data are in tumour.sav5.",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session15-16/assignment.html#q1-histogram-of-tumor-sizes",
    "href": "session15-16/assignment.html#q1-histogram-of-tumor-sizes",
    "title": "Laboratory 08",
    "section": "Q1: Histogram of tumor sizes",
    "text": "Q1: Histogram of tumor sizes\nGroup 1: 0 hours daily exposure on phone microwave (the control group);\nGroup 2-6: 1 hour - 5 hours of daily exposure on phone microwave.\nExamination of a histogram shown in Figure 1 of tumor sizes indicate that the scores were approximately normally distributed with no extreme outliers.\n\n\n\n\n\n\n\n\nFigure 1: Distribution of tumour sizes per group.",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session15-16/assignment.html#q2-hypothesis-tested-with-a-priori-comparisons",
    "href": "session15-16/assignment.html#q2-hypothesis-tested-with-a-priori-comparisons",
    "title": "Laboratory 08",
    "section": "Q2: Hypothesis tested with a priori comparisons",
    "text": "Q2: Hypothesis tested with a priori comparisons\nUnfortunately there’s no straightforward way to the planned contrast with Python modules. So this part I will switch to R.\n\nQ2a: the overall ANOVA result\nGiven:\n\\(H_0\\): The amount of daily exposure to phone microwaves does not affect the size of the tumor.\nResult:\nThe one-way ANOVA test (Table 2) showed a significant effect with \\(F(5, \\ 114) = 269.73\\), \\(p &lt; 0.001\\). However, the result of Levene’s test (see Table 1) suggested that the assumption of homogeneity of variance has been violated with \\(F(5,\\ 114) = 10.2453\\), \\(p &lt; 0.001\\), we should seek for Welch ANOVA for a more robust result. The Welch \\(F\\) result (Table 3) showed that using mobile phone significantly affected the size of brain tumor size \\(F(5,\\ 44.39) = 414.93\\), \\(p &lt; 0.001\\), indicating that exposure does affect tumor size. The null hypothesis (\\(H_0\\)) was therefore rejected.\n\n\n\n\nTable 1: Levene Test of Homogeneity of Variances\n\n\n\n\n\n\nW\npval\nequal_var\n\n\n\n\n10.2453\n3.97382e-08\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: One-way ANOVA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\nDF\nMS\nF\np-unc\nnp2\n\n\n\n\nusage\n450.664\n5\n90.1328\n269.733\n2.00779e-61\n0.92206\n\n\nWithin\n38.0938\n114\n0.334156\nnan\nnan\nnan\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3: Welch ANOVA\n\n\n\n\n\n\nSource\nddof1\nddof2\nF\np-unc\nnp2\n\n\n\n\nusage\n5\n44.3903\n414.926\n4.52854e-36\n0.92206\n\n\n\n\n\n\n\n\n\n\nQ2b: comparing 0 hours vs. 1 to 5 hours combined\nGiven:\nTo conduct an a priori comparisons for 0 hours vs. 1 to 5 hours combined, the null hypothesis \\(H_0\\) would be:\n\\(H_0: \\mu_1 - \\frac{\\mu_2 + \\mu_3 + \\mu_4 + \\mu_5 + \\mu_6}{5} = 0\\)\nThe contrast coefficients should be \\((+5, -1, -1, -1, -1, -1)\\).\nResult:\nPlanned contrast comparing the mean of Group 1, no exposure to the microwave, with the combined means of Groups 2–6, the microwave intervention groups was performed. This contrast was tested using \\(\\alpha = 0.05\\), two tailed. For this contrast, \\(t(114) = -20.24\\), \\(𝑝 &lt; 0.001\\), two tailed (or \\(t(75.37) = -45.22\\), \\(𝑝 &lt; 0.001\\) if equal variance not assumed). The mean the tumor size for the control group (\\(M = 0.02\\)) was significantly lower than the mean tumor size for the five combined intervention groups (\\(M = 2.41\\)).\n\n\nQ2c: the linear trend\nGiven:\nTo test whether the scores increase linearly, contrast coefficients should be \\((-5, -3, -1, +1, +3, +5)\\)\nResult\nIn contrast 2, a significant linear trend was shown with \\(t(114) = 35.55\\), \\(p &lt; 0.001\\), two-tailed (or \\(t(33.70) = 37.34\\), \\(p &lt; 0.001\\) if equal variance not assumed), indicating that tumor size increases linearly with more hours of exposure.\n contrast estimate    SE  df t.ratio p.value\n 0 vs 1-5    -14.3 0.708 114 -20.239  &lt;.0001\n linear       38.4 1.080 114  35.549  &lt;.0001",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session15-16/assignment.html#q3-post-hoc-tests",
    "href": "session15-16/assignment.html#q3-post-hoc-tests",
    "title": "Laboratory 08",
    "section": "Q3: Post hoc tests",
    "text": "Q3: Post hoc tests\n\nQ3a: The standard Tukey HSD\nOn the Levene’s test we have \\(F(5,\\ 114) = 10.2453\\), \\(p &lt; 0.001\\). There’s already a violation on the homogeneity of variances assumption. Should I do this?\nResults:\nAll possible pairwise comparisons were made using the Tukey HSD (as listed in Table 4). On \\(\\alpha = 0.05\\), significant differences were found between most pairs of groups. The tumor sizes for 0 hours differ significantly from all other exposure groups except the group of 1 hour (\\(p = 0.0790\\)). While tumor sizes for 1 hour differ significantly from 2 hours and higher. No significant difference between 4 and 5 hours (\\(p = 0.9551\\)). Overall, tumor size increases with more exposure.\n\n\n\n\nTable 4: Pairwise comparisons with Tukey HSD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nmean(A)\nmean(B)\ndiff\nse\nT\np-tukey\nhedges\n\n\n\n\n1\n2\n0.01755\n0.514886\n-0.497336\n0.182799\n-2.72067\n0.078988\n-2.4235\n\n\n1\n3\n0.01755\n1.26136\n-1.24381\n0.182799\n-6.80425\n7.42325e-09\n-3.50189\n\n\n1\n4\n0.01755\n3.02159\n-3.00404\n0.182799\n-16.4336\n2.26485e-14\n-5.43844\n\n\n1\n5\n0.01755\n4.8878\n-4.87025\n0.182799\n-26.6426\n2.26485e-14\n-9.6943\n\n\n1\n6\n0.01755\n4.73059\n-4.71304\n0.182799\n-25.7826\n2.26485e-14\n-8.357\n\n\n2\n3\n0.514886\n1.26136\n-0.746476\n0.182799\n-4.08359\n0.00114113\n-1.82059\n\n\n2\n4\n0.514886\n3.02159\n-2.50671\n0.182799\n-13.7129\n2.26485e-14\n-4.25493\n\n\n2\n5\n0.514886\n4.8878\n-4.37291\n0.182799\n-23.9219\n2.26485e-14\n-8.0601\n\n\n2\n6\n0.514886\n4.73059\n-4.21571\n0.182799\n-23.0619\n2.26485e-14\n-7.02603\n\n\n3\n4\n1.26136\n3.02159\n-1.76023\n0.182799\n-9.62931\n2.58682e-14\n-2.68085\n\n\n3\n5\n1.26136\n4.8878\n-3.62643\n0.182799\n-19.8383\n2.26485e-14\n-5.89535\n\n\n3\n6\n1.26136\n4.73059\n-3.46923\n0.182799\n-18.9784\n2.26485e-14\n-5.20611\n\n\n4\n5\n3.02159\n4.8878\n-1.8662\n0.182799\n-10.209\n2.27596e-14\n-2.49974\n\n\n4\n6\n3.02159\n4.73059\n-1.709\n0.182799\n-9.34905\n3.64153e-14\n-2.16517\n\n\n5\n6\n4.8878\n4.73059\n0.157201\n0.182799\n0.859967\n0.955111\n0.208165\n\n\n\n\n\n\n\n\n\n\nQ3b: Games-Howell\nGiven the equal variance assumption is violated and the Welch ANOVA was tested instead of the classic one-way ANOVA in the former section, the Games-Howell test seems to be much more optimal than HSD as it is much more robust to the heterogeneity of variances.\nResults:\nGames–Howell test revealed significant differences between all groups (\\(p &lt; 0.001\\), as shown in Table 5) for all groups except between 4 and 5 hours (group 5 and 6).\n\n\n\n\nTable 5: Pairwise comparisons with Games-Howell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nmean(A)\nmean(B)\ndiff\nse\nT\ndf\npval\nhedges\n\n\n\n\n1\n2\n0.01755\n0.514886\n-0.497336\n0.0636051\n-7.81912\n19.0692\n3.05245e-06\n-2.4235\n\n\n1\n3\n0.01755\n1.26136\n-1.24381\n0.110087\n-11.2984\n19.0231\n9.62253e-09\n-3.50189\n\n\n1\n4\n0.01755\n3.02159\n-3.00404\n0.171205\n-17.5465\n19.0095\n4.67837e-12\n-5.43844\n\n\n1\n5\n0.01755\n4.8878\n-4.87025\n0.155711\n-31.2775\n19.0115\n7.77156e-16\n-9.6943\n\n\n1\n6\n0.01755\n4.73059\n-4.71304\n0.174798\n-26.9628\n19.0091\n1.9984e-15\n-8.357\n\n\n2\n3\n0.514886\n1.26136\n-0.746476\n0.127083\n-5.87392\n30.4022\n2.6158e-05\n-1.82059\n\n\n2\n4\n0.514886\n3.02159\n-2.50671\n0.182598\n-13.728\n24.1391\n9.6545e-12\n-4.25493\n\n\n2\n5\n0.514886\n4.8878\n-4.37291\n0.168157\n-26.0049\n25.16\n0\n-8.0601\n\n\n2\n6\n0.514886\n4.73059\n-4.21571\n0.185971\n-22.6686\n23.9373\n0\n-7.02603\n\n\n3\n4\n1.26136\n3.02159\n-1.76023\n0.203508\n-8.64943\n32.4145\n9.08664e-09\n-2.68085\n\n\n3\n5\n1.26136\n4.8878\n-3.62643\n0.190658\n-19.0206\n34.1944\n0\n-5.89535\n\n\n3\n6\n1.26136\n4.73059\n-3.46923\n0.20654\n-16.7969\n32.0201\n3.88578e-15\n-5.20611\n\n\n4\n5\n3.02159\n4.8878\n-1.8662\n0.231392\n-8.0651\n37.6629\n1.46334e-08\n-2.49974\n\n\n4\n6\n3.02159\n4.73059\n-1.709\n0.244644\n-6.98566\n37.9836\n3.73845e-07\n-2.16517\n\n\n5\n6\n4.8878\n4.73059\n0.157201\n0.234063\n0.67162\n37.5028\n0.984024\n0.208165",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session15-16/assignment.html#q4-the-box-plot",
    "href": "session15-16/assignment.html#q4-the-box-plot",
    "title": "Laboratory 08",
    "section": "Q4: The box-plot",
    "text": "Q4: The box-plot\nThe boxplot is shown as Figure 2, which visually supports the findings above, showing a significant and linear increase in tumor size with more mobile phone usage, while also highlighting the variance issue across groups. The boxplot shows a increase in median tumor size with more mobile phone usage, which aligns with the statistical analysis. Besides, the spread of data increases with more exposure, and there are a few outliers, particularly in the 5-hour group. However, The increase in tumor size appears to plateau between 4 and 5 hours, which reflects the Tukey HSD results on showing no significant difference between these groups.\n\n\n\n\n\n\n\n\nFigure 2: Boxplots for tumour size by groups of mobile phone usage",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session15-16/assignment.html#q5-assumptions-of-anova",
    "href": "session15-16/assignment.html#q5-assumptions-of-anova",
    "title": "Laboratory 08",
    "section": "Q5: Assumptions of ANOVA",
    "text": "Q5: Assumptions of ANOVA\nThe significant Levene’s test (\\(p &lt; 0.001\\)) and the error bar of 95% CI (see Figure 3) reveals there was little variance across samples, indicating a violation of the assumption on equal variances. So both the standard one-way ANOVA and Tukey HSD test, which assumes equal variances, might not be the most appropriate choice. Although the large sample sizes in each group (\\(n = 20\\)) might help for this issue. The results should be interpreted with caution, and alternative methods (in this lab work the Welch ANOVA and Games-Howell test was selected) could be considered for more robustness.\n\n\n\n\n\n\n\n\nFigure 3: Error bars: 95% CI",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session15-16/assignment.html#footnotes",
    "href": "session15-16/assignment.html#footnotes",
    "title": "Laboratory 08",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe IRB would never approve this. It would make participants look more foolish than the TOEFL headphone man.↩︎\n6 month?! The research team must have compensated all participants (maybe a guaranteed tenure).↩︎\nIn my work, I use “tumor” instead of “tumour” because I have a personal aversion to EN-GB for some reason.↩︎\nEven patients in a square-cabin hospital receive better treatment than this experiment.↩︎\nI finally realized that this dataset came from Andy Field’s statistics textbook. Chester frequently referenced this book during our master’s coursework. This kind of edgy, almost punk-like experiment could only be conceived by a British rock star.↩︎\nBut many interestingly designed phones emerged in 2003, like the Siemens SX1 and the original Nokia N-Gage. Those were the good old days!↩︎",
    "crumbs": [
      "Labs",
      "Session15 16",
      "Laboratory 08"
    ]
  },
  {
    "objectID": "session02/lecture-notes.html",
    "href": "session02/lecture-notes.html",
    "title": "Section 02: Concepts",
    "section": "",
    "text": "Brief review on:\n\nimport data on SPSS\ntypes of variable (i.e. measurement) in SPSS.\nSPSS syntax - a “code-like” thing to replicate what we have done\n\nResearch Questions and Design: showed general concepts future scholars must learn before (otherwise how could they get into the graduate school)"
  },
  {
    "objectID": "session02/lecture-notes.html#summary",
    "href": "session02/lecture-notes.html#summary",
    "title": "Section 02: Concepts",
    "section": "",
    "text": "Brief review on:\n\nimport data on SPSS\ntypes of variable (i.e. measurement) in SPSS.\nSPSS syntax - a “code-like” thing to replicate what we have done\n\nResearch Questions and Design: showed general concepts future scholars must learn before (otherwise how could they get into the graduate school)"
  },
  {
    "objectID": "session02/lecture-notes.html#research-questions",
    "href": "session02/lecture-notes.html#research-questions",
    "title": "Section 02: Concepts",
    "section": "Research Questions",
    "text": "Research Questions\n\nVariables and causal relationships\n\nthe X-axis and Y-axis - assuming \\(X\\) will affect \\(Y\\)\n\n\\(X\\) =&gt; Independent var\n\n\\(Y\\) =&gt; Dependent var\n\n\nare \\(X\\) and \\(Y\\) correlated or associated?\ndoes \\(X\\) predict \\(Y\\)?\ndoes \\(X\\) cause/determine/influence \\(Y\\)? - strong evidence needed\n\n\nConfounding\n\nassociated with the \\(X\\) that cause or influence \\(Y\\)\nmay cause false association\ne.g. ice cream sales vs drowning incidents\n\naffected by confounder - temperature - another variable makes \\(X\\) and \\(Y\\) correlated.\n\ne.g. age vs wages\n\nage - associated with experience + wages\n\nControls:\n\nExperimental\nStatistical\n\n\n\n\nMediation\n\ne.g. wages - male vs female\n\nfrom average wages: yes they are correlated.\nbut: does it imply discrimination?\nlook deeper: control of occupations\n\nSimpson’s paradox\n\ngender –occupation–&gt; wages\n\n\n\n\n\nAddressing causal inferences\n\nTheory\nStatistically related\nTime: \\(X\\) happens earlier\nThe causal inferences: proof no rival explanations for the changes of \\(Y\\).\n\n(Brannon et al., 2017; Cozby & Bates, 2017)"
  },
  {
    "objectID": "session02/lecture-notes.html#research-design",
    "href": "session02/lecture-notes.html#research-design",
    "title": "Section 02: Concepts",
    "section": "Research Design",
    "text": "Research Design\n\nExperiment\n\ngrouping and control\ncontrol group vs treatment group\nrandomized, standardized, clear-defined dependent variable\nRCT - Randomized controlled trial: cost, ethic, external validity\n\n\n\nNon-experiment\n\ncorrelational research: e.g. questionnaires\ncost: generally **doesn’t lead to causality”\n\ntemporal precedence + confounding\ncorrelations \\(\\neq\\) causation\n\n\n\n\nQuasi-experiment\n\nuse pre-existing groups - Nonequivalent control group\n\ne.g. free lunch in school 1 but not in school 2\nconfounding my exists\n\nuse data before and after the intervention - One-group pretest-post-test design\n\ne.g. maturation\n\n\n\n\nComparison of designs\n\ninternal validity vs external validity\ntrade-offs"
  },
  {
    "objectID": "session02/lecture-notes.html#populations-and-samples",
    "href": "session02/lecture-notes.html#populations-and-samples",
    "title": "Section 02: Concepts",
    "section": "Populations and Samples",
    "text": "Populations and Samples"
  },
  {
    "objectID": "session16-17/assignment.html",
    "href": "session16-17/assignment.html",
    "title": "Laboratory 09",
    "section": "",
    "text": "Have you heard of the beer-goggles effect? It predicts that subjective perceptions of physical attractiveness become inaccurate after drinking alcohol. The logic is that alcohol consumption has been shown to reduce accuracy in symmetry judgement, and symmetric faces have been shown to be rated as more attractive. If the beer-goggles effect is driven by alcohol impairing symmetry judgement then you would expect a stronger effect for unattractive (asymmetric) faces (because alcohol will affect the perception of asymmetry) than attractive (symmetric) ones.\nAn anthropologist was interested in the effects of facial attractiveness on the beer-goggles effect. She randomly selected 48 participants. Participants were randomly subdivided into three groups of 16:\n\na placebo group drank 500 ml of alcohol-free beer;\na low-dose group drank 500 ml of average strength beer (4% ABV); and\na high-dose group drank 500 ml of strong beer (7% ABV).\n\nWithin each group, half (\\(n =8\\)) rated the attractiveness of 50 photos of unattractive (asymmetric) faces on a scale from 0 (pass me a paper bag) to 10 (pass me their phone number) and the remaining half rated 50 photos of attractive (symmetric) faces. The outcome for each participant was their median rating across the 50 photos. The data are in goggles.sav.\n\n\n\n\nConduct a factorial ANOVA to study the effects of facial attractiveness and alcohol consumption on the rating. Report the results of tests for main effects and interaction effects, and their corresponding partial \\(\\eta_p^2\\).\nDraw a line chart using the ⟨ Plots ⟩ option. Let ⟨ Horizontal Axis ⟩ and ⟨ Separate Lines ⟩ be alcohol consumption and facial attractiveness respectively, and include error bars showing the 95% confidence intervals of the means.\nSummarize your findings from the analyses.\n\n\n\n\n\n\nJust mention that the Levene’s test being not significant indicates that the assumption of homogeneity of variance is satisfied (as you expected). So, based on the results from the factorial ANOVA:\nA \\(3 \\times 2\\) factorial ANOVA was performed using the SPSS GLM procedure to assess whether the attractiveness rating \\(Y\\) could be predicted from level of alcohol consumption (\\(A_1 =\\) Placebo, \\(A_2 =\\) Low dose, \\(A_3 =\\) High dose), facial attractiveness (\\(B_1 =\\) Unattractive, \\(B_2 =\\) Attractive), and the interaction between doses of alcohol and the facial attractiveness.\n\n\n\nTable 1: \\(3 \\times 2\\) factorial ANOVA (RANKING by ALCOHOL and FACETYPE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\\(\\eta_p^2\\)\n\n\n\n\nALCOHOL\n16.542\n2\n8.271\n6.041\n.005\n.223\n\n\nFACETYPE\n21.333\n1\n21.333\n15.583\n&lt;.001\n.271\n\n\n\\(A \\times F\\)\n23.292\n2\n11.646\n8.507\n&lt;.001\n.288\n\n\nError\n57.500\n42\n1.369\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The interaction of type of face and alcohol consumption on the ratings\n\n\n\nThere is a significant main effect of alcohol consumption on attractiveness ratings, \\(F_A(2, 42) = 6.041\\), \\(p = 0.005\\), with a \\(\\eta_p^2 = 0.223\\), indicating that alcohol consumption influences attractiveness ratings. As alcohol consumption increases, ratings generally change, especially between low and high doses.\nThe main effect on facial attractiveness on attractiveness ratings is significant as well, \\(F_B(1, 42) = 15.583\\), \\(p &lt; 0.001\\), with a \\(\\eta_p^2 = 0.271\\), showing that symmetric (attractive) faces are rated higher than asymmetric (unattractive) faces across all alcohol consumption levels.\nFor the interaction effect, there is a significant interaction between alcohol consumption and facial attractiveness, \\(F_{A \\times B}(2, 42) = 8.507\\), \\(p &lt; 0.001\\), with a \\(\\eta_p^2 = 0.288\\). The beer-goggles effect is more pronounced for unattractive faces (see Figure 1): ratings increase significantly as alcohol consumption increases from placebo to high dose. In contrast, attractive faces maintain relatively consistent ratings across alcohol levels.\nAdditionally, Figure 1 illustrates that ratings for unattractive faces improve significantly from placebo to high-dose conditions, highlighting the interaction effect. Attractive faces show smaller changes, supporting that alcohol affects symmetry perception more in unattractive faces.\nOverall, the findings suggest that both alcohol consumption and facial attractiveness independently influence attractiveness ratings, and their interaction further impacts these ratings.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#context",
    "href": "session16-17/assignment.html#context",
    "title": "Laboratory 09",
    "section": "",
    "text": "Have you heard of the beer-goggles effect? It predicts that subjective perceptions of physical attractiveness become inaccurate after drinking alcohol. The logic is that alcohol consumption has been shown to reduce accuracy in symmetry judgement, and symmetric faces have been shown to be rated as more attractive. If the beer-goggles effect is driven by alcohol impairing symmetry judgement then you would expect a stronger effect for unattractive (asymmetric) faces (because alcohol will affect the perception of asymmetry) than attractive (symmetric) ones.\nAn anthropologist was interested in the effects of facial attractiveness on the beer-goggles effect. She randomly selected 48 participants. Participants were randomly subdivided into three groups of 16:\n\na placebo group drank 500 ml of alcohol-free beer;\na low-dose group drank 500 ml of average strength beer (4% ABV); and\na high-dose group drank 500 ml of strong beer (7% ABV).\n\nWithin each group, half (\\(n =8\\)) rated the attractiveness of 50 photos of unattractive (asymmetric) faces on a scale from 0 (pass me a paper bag) to 10 (pass me their phone number) and the remaining half rated 50 photos of attractive (symmetric) faces. The outcome for each participant was their median rating across the 50 photos. The data are in goggles.sav.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#objectives",
    "href": "session16-17/assignment.html#objectives",
    "title": "Laboratory 09",
    "section": "",
    "text": "Conduct a factorial ANOVA to study the effects of facial attractiveness and alcohol consumption on the rating. Report the results of tests for main effects and interaction effects, and their corresponding partial \\(\\eta_p^2\\).\nDraw a line chart using the ⟨ Plots ⟩ option. Let ⟨ Horizontal Axis ⟩ and ⟨ Separate Lines ⟩ be alcohol consumption and facial attractiveness respectively, and include error bars showing the 95% confidence intervals of the means.\nSummarize your findings from the analyses.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#solutions-q1-q3-combined",
    "href": "session16-17/assignment.html#solutions-q1-q3-combined",
    "title": "Laboratory 09",
    "section": "",
    "text": "Just mention that the Levene’s test being not significant indicates that the assumption of homogeneity of variance is satisfied (as you expected). So, based on the results from the factorial ANOVA:\nA \\(3 \\times 2\\) factorial ANOVA was performed using the SPSS GLM procedure to assess whether the attractiveness rating \\(Y\\) could be predicted from level of alcohol consumption (\\(A_1 =\\) Placebo, \\(A_2 =\\) Low dose, \\(A_3 =\\) High dose), facial attractiveness (\\(B_1 =\\) Unattractive, \\(B_2 =\\) Attractive), and the interaction between doses of alcohol and the facial attractiveness.\n\n\n\nTable 1: \\(3 \\times 2\\) factorial ANOVA (RANKING by ALCOHOL and FACETYPE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScore\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\\(\\eta_p^2\\)\n\n\n\n\nALCOHOL\n16.542\n2\n8.271\n6.041\n.005\n.223\n\n\nFACETYPE\n21.333\n1\n21.333\n15.583\n&lt;.001\n.271\n\n\n\\(A \\times F\\)\n23.292\n2\n11.646\n8.507\n&lt;.001\n.288\n\n\nError\n57.500\n42\n1.369\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The interaction of type of face and alcohol consumption on the ratings\n\n\n\nThere is a significant main effect of alcohol consumption on attractiveness ratings, \\(F_A(2, 42) = 6.041\\), \\(p = 0.005\\), with a \\(\\eta_p^2 = 0.223\\), indicating that alcohol consumption influences attractiveness ratings. As alcohol consumption increases, ratings generally change, especially between low and high doses.\nThe main effect on facial attractiveness on attractiveness ratings is significant as well, \\(F_B(1, 42) = 15.583\\), \\(p &lt; 0.001\\), with a \\(\\eta_p^2 = 0.271\\), showing that symmetric (attractive) faces are rated higher than asymmetric (unattractive) faces across all alcohol consumption levels.\nFor the interaction effect, there is a significant interaction between alcohol consumption and facial attractiveness, \\(F_{A \\times B}(2, 42) = 8.507\\), \\(p &lt; 0.001\\), with a \\(\\eta_p^2 = 0.288\\). The beer-goggles effect is more pronounced for unattractive faces (see Figure 1): ratings increase significantly as alcohol consumption increases from placebo to high dose. In contrast, attractive faces maintain relatively consistent ratings across alcohol levels.\nAdditionally, Figure 1 illustrates that ratings for unattractive faces improve significantly from placebo to high-dose conditions, highlighting the interaction effect. Attractive faces show smaller changes, supporting that alcohol affects symmetry perception more in unattractive faces.\nOverall, the findings suggest that both alcohol consumption and facial attractiveness independently influence attractiveness ratings, and their interaction further impacts these ratings.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#context-1",
    "href": "session16-17/assignment.html#context-1",
    "title": "Laboratory 09",
    "section": "Context",
    "text": "Context\nThe researcher took a follow-up sample of 26 people and gave them doses of alcohol (0 pints, 2 pints, 4 pints and 6 pints of lager) over four different weeks. She asked them to rate a bunch of photos of unattractive faces in either dim or bright lighting. The outcome measure was the mean attractiveness rating (out of 100) of the faces and the predictors were the dose of alcohol and the lighting conditions. The data are in goggles_lighting.sav.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#objectives-1",
    "href": "session16-17/assignment.html#objectives-1",
    "title": "Laboratory 09",
    "section": "Objectives",
    "text": "Objectives\n\nConduct a repeated-measures ANOVA to study whether the dose of alcohol affects the mean attractiveness rating in bright lighting. Report the result of the test and the corresponding effect size.\nDraw a line chart using the ⟨ Plots ⟩ option. Let ⟨ Horizontal Axis ⟩ be alcohol consumption, and include error bars showing the 95% confidence intervals of the means.\nSummarize your findings from the analyses.\n(Extra credit) Test whether alcohol dose and lighting interact to magnify the beer goggles effect. In SPSS, we can add another factor, lighting with two levels, to the repeated-measures ANOVA procedure in SPSS.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#solutions",
    "href": "session16-17/assignment.html#solutions",
    "title": "Laboratory 09",
    "section": "",
    "text": "Just mention that the Levene’s test being not significant indicates that the assumption of homogeneity of variance is satisfied (as you expected). So, based on the results from the factorial ANOVA:\nA \\(3 \\times 2\\) factorial ANOVA was performed using the SPSS GLM procedure to assess whether the attractiveness rating \\(Y\\) could be predicted from level of alcohol consumption (\\(A_1 =\\) Placebo, \\(A_2 =\\) Low dose, \\(A_3 =\\) High dose), facial attractiveness (\\(B_1 =\\) Unattractive, \\(B_2 =\\) Attractive), and the interaction between doses of alcohol and the facial attractiveness.\n\n\n\nTable 1: \\(3 \\times 2\\) factorial ANOVA (RANKING by ALCOHOL and FACETYPE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\\(\\eta_p^2\\)\n\n\n\n\nALCOHOL\n16.542\n2\n8.271\n6.041\n.005\n.223\n\n\nFACETYPE\n21.333\n1\n21.333\n15.583\n&lt;.001\n.271\n\n\n\\(A \\times F\\)\n23.292\n2\n11.646\n8.507\n&lt;.001\n.288\n\n\nError\n57.500\n42\n1.369\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: The interaction of type of face and alcohol consumption on the ratings\n\n\n\nThere is a significant main effect of alcohol consumption on attractiveness ratings, \\(F_A(2, 42) = 6.041\\), \\(p = 0.005\\), with a \\(\\eta_p^2 = 0.223\\), indicating that alcohol consumption influences attractiveness ratings. As alcohol consumption increases, ratings generally change, especially between low and high doses.\nThe main effect on facial attractiveness on attractiveness ratings is significant as well, \\(F_B(1, 42) = 15.583\\), \\(p &lt; 0.001\\), with a \\(\\eta_p^2 = 0.271\\), showing that symmetric (attractive) faces are rated higher than asymmetric (unattractive) faces across all alcohol consumption levels.\nFor the interaction effect, there is a significant interaction between alcohol consumption and facial attractiveness, \\(F_{A \\times B}(2, 42) = 8.507\\), \\(p &lt; 0.001\\), with a \\(\\eta_p^2 = 0.288\\). The beer-goggles effect is more pronounced for unattractive faces (see Figure 1): ratings increase significantly as alcohol consumption increases from placebo to high dose. In contrast, attractive faces maintain relatively consistent ratings across alcohol levels.\nAdditionally, Figure 1 illustrates that ratings for unattractive faces improve significantly from placebo to high-dose conditions, highlighting the interaction effect. Attractive faces show smaller changes, supporting that alcohol affects symmetry perception more in unattractive faces.\nOverall, the findings suggest that both alcohol consumption and facial attractiveness independently influence attractiveness ratings, and their interaction further impacts these ratings.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session13-14/assignment.html",
    "href": "session13-14/assignment.html",
    "title": "Laboratory 07",
    "section": "",
    "text": "Finally I noticed something on the requirement of all lab assignments I have ignored from the very beginning:\nI therefore turned echo for few code block to false. The long and annoying code must be interruptive while reading.",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#context",
    "href": "session13-14/assignment.html#context",
    "title": "Laboratory 07",
    "section": "Context",
    "text": "Context\nIn Laboratory 5 we analyzed tea.sav, which contains cognitive function scores of participants who consumed varying amounts of tea every day. Now we hope to study whether there is a difference in cognitive function scores between individuals who do not drink tea and those who drink two cups of tea per day.",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#objectives",
    "href": "session13-14/assignment.html#objectives",
    "title": "Laboratory 07",
    "section": "Objectives",
    "text": "Objectives\n\nDo you think the independent-samples \\(t\\)-test is a good choice? Why or why not?\nConduct an independent-samples \\(t\\)-test. Write a short paragraph that summarizes and interprets the results of the test.",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#solutions",
    "href": "session13-14/assignment.html#solutions",
    "title": "Laboratory 07",
    "section": "Solutions",
    "text": "Solutions\n\nQ1: Why independent-samples \\(t\\)-test?\nGiven:\n\\(X\\): cups of tea\n\\(Y\\): cognitive function scores\nGroup 1: not drink tea per day (tea == 0)\nGroup 2: 2 cups of tea per day (tea == 2)\nIn addition, in the description of Lab 5:\n\nThe researcher recruits a large group of respondents and randomly assigns each one with a different number of cups of tea to drink every day.\n\nBased on the assumption of independent-samples t-test and the design of the experiment, It might be a good choice for the following reason:\n\nContinuous Variable: The dependent variable, cognitive function scores, is continuous, which is suitable for an independent-samples \\(t\\)-test.\nNormal Distribution: The scores appear to follow a normal distribution, and with a large sample size, the Central Limit Theorem supports the normality assumption.\nIndependence Between Groups: Each participant belongs to only one group (either tea = 0 or tea = 2), ensuring independence of scores between groups.\nRandom Sampling: Random sampling ensures that scores are independent within groups, fulfilling this assumption of the independent-samples \\(t\\)-test.\n\nThese points may confirm that the independent-samples \\(t\\)-test is an appropriate statistical method for this analysis.\n\n\n\n\n\n\n\n\nFigure 1: Histograms on cognitive function for 0-and 2-cup-tea drinker\n\n\n\n\n\n\n\nQ2: Interpret the independent-samples \\(t\\)-test result.\nThe Levene’s Test resulted \\(F \\approx 0.0797\\) and a high \\(p \\approx 0.7781\\), indicated that there is no significant difference in variances between the groups, however, due to the unequal sample size, we may stick on the Welch’s \\(t\\)-test here. The result was shown as follows:\nIn the independent-samples \\(t\\)-test comparing cognitive function scores between individuals who do not drink tea (Group 1) and those who drink two cups of tea per day (Group 2), the results indicated a \\(t(89.65) \\approx 1.375\\), \\(p \\approx 0.1725\\), two-tailed, which suggests that there is no statistically significant difference in cognitive function scores between the two groups at the conventional level of \\(\\alpha = 0.05\\). The 95% confidence interval for the difference in means are \\([-1.11, 6.09]\\). The effect size, measured by Cohen’s \\(d \\approx 0.23\\), reflecting a small effect. Additionally, the Bayes factor \\(BF_{10} \\approx 0.432\\) suggests that the evidence is more in favor of the null hypothesis than the alternative. Overall, these findings imply that drinking two cups of tea per day does not significantly impact cognitive function compared to non-tea drinkers within the context of this experiment.\n\n\nLevene's Test for Equality of Variances: \nF: 0.07969429918151855\nSig: 0.7780595985739489\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\nWelch’s \\(t\\)-test applied\n1.37534\n89.6494\ntwo-sided\n0.172453\n[-1.11 6.09]\n0.229847\n0.432\n0.26855\n\n\nIndependent-samples \\(t\\)-test\n1.34901\n168\ntwo-sided\n0.17915\n[-1.15 6.14]\n0.229847\n0.418\n0.26855",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#context-1",
    "href": "session13-14/assignment.html#context-1",
    "title": "Laboratory 07",
    "section": "Context",
    "text": "Context\nIn Laboratory 1 we analyzed download.sav, which contains hygiene scores of concert goers over three days of a rock music festival. Now we hope to study the relationship between hygiene scores on days 2 and 3.",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#objectives-1",
    "href": "session13-14/assignment.html#objectives-1",
    "title": "Laboratory 07",
    "section": "Objectives",
    "text": "Objectives\n\nIs the correlation coefficient an appropriate measure of the relationship between hygiene scores on days 2 and 3? Why or why not?\nConduct a paired-samples 𝑡 test. Write a short paragraph that summarizes and interprets the results of the test.",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#solutions-1",
    "href": "session13-14/assignment.html#solutions-1",
    "title": "Laboratory 07",
    "section": "Solutions",
    "text": "Solutions\nYou may still remember the Ms. Laundry Pod the Outlier in the story of Lab 1. The bad news is she’s still living in the provided dataset. While the good news is this typo only happens on day_1 and since we are interested in the hygiene scores for day_2 and day_3 this time, we may not need to “say goodbye” as we did in Lab 1.\n良かったね、ランドリーポッドさん。1\nor should I say that?\nSince the assumption of paired sample \\(t\\)-test requires the same variable measured in the same units for the two situations, all participants without hygiene scores for either day_2 or day_3 should be dropped. The pingouin package will handle this automatically while doing the math, but not for plotting. Therefore, a new DataFrame was created with a new \\(N = 123\\).\nさよなら、ランドリーポッドさん。\n\n\n\n\n\nindex\nticket_no\nday_1\nday_2\nday_3\n\n\n\n\ncount\n123\n123\n123\n123\n\n\nmean\n3647.98\n1.65146\n0.92748\n0.976504\n\n\nstd\n626.302\n0.643897\n0.666004\n0.710277\n\n\nmin\n2111\n0.43\n0.05\n0.02\n\n\n25%\n3274.5\n1.16\n0.425\n0.44\n\n\n50%\n3649\n1.55\n0.78\n0.76\n\n\n75%\n4183\n2.095\n1.215\n1.525\n\n\nmax\n4724\n3.38\n3.44\n3.41\n\n\n\n\n\n\nQ3: Is correlation coefficient appropriate\nRecall back to the assumption of Pearson’s \\(r\\), the correlation coefficient is partially an appropriate measure to study the relationship between hygiene scores on days 2 and 3:\n\nSample Size: As suggested in the description of Lab1, the researcher had tested all comers with an \\(N = 123\\), which is relatively large and similar to the population of interest.\nLinear Relationship: The scatter plot on the relationship between hygiene scores on Day 2 and Day 3 (see Figure 2) shows a clear linear trend, indicating a positive linear relationship. This suggests that using a correlation coefficient to measure this relationship is appropriate.\nScale of Measurement: The hygiene scores are continuous variables, making the correlation coefficient suitable.\nPaired Observations: The data represents paired measurements from the same individuals.\nOutliers: There seems not appear to be any significant outliers that would severely distort the correlation.\n\nHowever: As for the normality: While there is some skewness (as shown by Figure 3), the relationship appears reasonably linear and without extreme outliers that would severely distort the correlation.\n\n\n\n\n\n\n\n\nFigure 2: Scatter-plot on hygiene scores between day 2 vs day 3\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Histograms on Hygiene Scores of Day 2 vs Day 3\n\n\n\n\n\n\n\nQ4: Interpret the paired-sample \\(t\\)-test result\nA paired sample \\(t\\)-test was conducted to compare the hygiene scores between days 2 and 3 of the Download Music Festival. The test yielded \\(t(122) \\approx -0.9685\\), \\(p \\approx 0.3347\\). This indicates that there is no statistically significant difference in hygiene scores between the two days. The 95% CI for the mean difference is \\([-0.15, 0.05]\\), suggesting that any difference is likely small and not meaningful. The effect size, as measured by Cohen’s \\(d \\approx 0.0712\\), indicating a negligible effect. The Bayes Factor \\(BF_{10} \\approx 0.158\\) suggests that the data provide more support for the null hypothesis than the alternative. Overall, the results suggest no substantial change in hygiene scores from day 2 to day 3 2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nT\ndof\nalternative\np-val\nCI95%\ncohen-d\nBF10\npower\n\n\n\n\n-0.968472\n122\ntwo-sided\n0.334725\n[-0.15 0.05]\n0.071205\n0.158\n0.122744",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session13-14/assignment.html#footnotes",
    "href": "session13-14/assignment.html#footnotes",
    "title": "Laboratory 07",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis sounds like an anime quote.↩︎\nThat’s definitely true for a music festival full of sweat, alcohol, Seishun (青春, that’s youth in Japanese) and romance! Only problem for these activities is they are too “リア充”, it’s a shame the bands there never appears to play Anisongs. There’s no random Togenashi Togeari (トゲナシトゲアリ) appear on the stage of summer music festival, at least you can’t count this happen in Greater China.  If you can’t figure out what I am saying, skip it TTATT.↩︎",
    "crumbs": [
      "Labs",
      "Session13 14",
      "Laboratory 07"
    ]
  },
  {
    "objectID": "session05-06/assignment.html",
    "href": "session05-06/assignment.html",
    "title": "Laboratory 03",
    "section": "",
    "text": "Thanks for the feedback on my Lab 2 Assignment! Here’s my response:\n\n\n\n“normally” have higher median?\n\nWhat I mean is that based on the sample and the box-plot, we can generally infer that “cat people tend to have a higher life satisfaction score” than fish owners.\nApologies for the earlier miswording and ambiguity in the interpretation.\n\n\n\nThey usually come out in the early morning and late at night, as cats normally do. However, they are all around at various times. I even once witnessed the police and firefighters at our faculty, rescuing a young cat trapped on the roof!",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#normally-on-line-4-section-3.2-question-2-page-5",
    "href": "session05-06/assignment.html#normally-on-line-4-section-3.2-question-2-page-5",
    "title": "Laboratory 03",
    "section": "",
    "text": "“normally” have higher median?\n\nWhat I mean is that based on the sample and the box-plot, we can generally infer that “cat people tend to have a higher life satisfaction score” than fish owners.\nApologies for the earlier miswording and ambiguity in the interpretation.",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#about-the-cats",
    "href": "session05-06/assignment.html#about-the-cats",
    "title": "Laboratory 03",
    "section": "",
    "text": "They usually come out in the early morning and late at night, as cats normally do. However, they are all around at various times. I even once witnessed the police and firefighters at our faculty, rescuing a young cat trapped on the roof!",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#a-peek-on-the-dataset",
    "href": "session05-06/assignment.html#a-peek-on-the-dataset",
    "title": "Laboratory 03",
    "section": "A peek on the dataset",
    "text": "A peek on the dataset\nAs usual, I load modules that I may need in this laboratory assignment, then the dataset to my RAM and check attributes of the given dataset.\n\nimport pandas as pd\n\n# Load the dataset\nswitch = pd.read_spss('./datasets/switch.sav')\n\n\n# Descriptions\nprint(f'Shape: \\n', switch.shape, '\\n')\nprint(f'Columns: \\n', switch.columns, '\\n')\nprint(f'First 5 rows: \\n', switch.head(5), '\\n')\nprint(f'Describe the column `injury`: \\n', switch.describe(), '\\n')\n\nShape: \n (120, 5) \n\nColumns: \n Index(['id', 'athlete', 'stretch', 'switch', 'injury'], dtype='object') \n\nFirst 5 rows: \n     id  athlete     stretch          switch  injury\n0  ytv  Athlete  Stretching  Playing switch     2.0\n1  wel  Athlete  Stretching  Playing switch     2.0\n2  qfs  Athlete  Stretching  Playing switch     1.0\n3  oln  Athlete  Stretching  Playing switch     2.0\n4  wxi  Athlete  Stretching  Playing switch     0.0 \n\nDescribe the column `injury`: \n            injury\ncount  120.000000\nmean     2.891667\nstd      1.994934\nmin      0.000000\n25%      2.000000\n50%      2.000000\n75%      4.000000\nmax     10.000000",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#q1-distribution-of-the-pain-scores",
    "href": "session05-06/assignment.html#q1-distribution-of-the-pain-scores",
    "title": "Laboratory 03",
    "section": "Q1: Distribution of the pain scores",
    "text": "Q1: Distribution of the pain scores\n\nQ1a: Describe the distribution\nAnswer\nTo describe the distribution of the pain scores, I use histogram with a kernel density estimation curve as shown in Figure 1 as well as measurements (mean, mode and median) reflect central tendency (see Table 1).\n\n\n\nTable 1: Mean, mode and median\n\n\n\n\n\nMeasurement\nValue\n\n\n\n\nMode\n2.00\n\n\nMean\n2.89\n\n\nMedian\n2.00\n\n\n\n\n\n\nAccording to the graph:\n\nMost of the observations are clustered around the lower pain scores (between 1 and 4), we can say that the distribution of pain scores is positively skewed rather than a perfect normal distribution.\nThere is a noticeable peak at a score of 2, which means the most frequent score is around 2.\nA long tail extends to the higher scores, indicating the frequency of pain scores gradually decreases as the scores increase.\n\nSolution\n\ninjury = switch['injury']\n\n# Calculate measurements of central tendency\ninjury_mean = injury.mean()\ninjury_mode = injury.mode()[0]\ninjury_median = injury.median()\n\n# Tell the result\nprint(f'Central Tendency: \\n')\nprint(f'Mean: ', injury_mean)\nprint(f'Mode: ', injury_mode)\nprint(f'Median: ', injury_median)\n\nCentral Tendency: \n\nMean:  2.8916666666666666\nMode:  2.0\nMedian:  2.0\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot the histogram\ninjury_hist = sns.histplot(switch, x='injury', stat='count', bins=10 ,kde=True)\n# Dashed line for Mean, Median and Mode\ninjury_hist.axvline(injury_mean, color='blue', linestyle='--', linewidth=1)\ninjury_hist.axvline(injury_median, color='red', linestyle='--', linewidth=1)\n# Set title and labels\ninjury_hist.set_title('Distribution of the pain scores')\ninjury_hist.set_xlabel('Pain score (out of 10)')\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Distribution of the pain scores\n\n\n\n\n\n\n\nQ1b: One-sample \\(𝑡\\) test on this variable\nAnswer\nRecall back the the slides in the lecture notes:\n\nOne‐sample \\(t\\) test requires that:\n\nSample mean describes central tendency.\n\n\nThe sample mean is slightly higher than the mode and median (see Table 1 and Figure 1, red dashed lines for the median and mode, blue for the mean) since the data is right-skewed. However, they are fairly close to each other, so the sample mean can still represent central tendency.\n\n\nScores in the sample are randomly selected from the population\n\n\nAccording to the description, the data was “collected from 120 participants who played on a Nintendo Switch or watched others playing.” For the sake of this assignment, I will assume that the participants were randomly selected from patients worldwide to fulfill the random sampling assumption.\n\n\nEither \\(N\\) is large or \\(X\\) follows a normal distribution\n\n\nGiven the right-skewed distribution as seen in Figure 1, the data may violate the assumption of normality required for the one-sample \\(t\\)-test. However, the Central Limit Theorem suggests that if the sample size is large (typically \\(N &gt; 30\\)), the sampling distribution of the sample mean tends to approach normality. Therefore, despite the skewed distribution, the sample size (\\(N = 120\\)) makes the one-sample \\(t\\)-test acceptable in this case.\nAdditionally, question 2 specifically asks for a one-sample \\(t\\)-test without requiring further preprocessing of data (e.g., a log transformation), which further supports the applicability of the one-sample \\(t\\)-test to this data. If the data were unusable, there would be no reason to include the following questions.\nIn conclusion, a one-sample \\(t\\)-test is suitable for this dataset.",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#q2-mean-of-pain-score-tested",
    "href": "session05-06/assignment.html#q2-mean-of-pain-score-tested",
    "title": "Laboratory 03",
    "section": "Q2: Mean of pain score tested",
    "text": "Q2: Mean of pain score tested\nAnswer\n\n\\(p\\) value\n\\(p \\approx 3.11 \\times 10^{-6}\\)\nAt a 5% significance level (\\(\\alpha = 0.05\\)), \\(p &lt; 0.001\\), we reject the null hypothesis. The mean pain score is significantly different from 2 (a minor pain) at the 5% level.\nThe Cohen’s \\(d\\) value\n\\(d \\approx 0.45\\)\nThe Cohen’s \\(d\\) value indicates a medium effect. This suggests that the difference between the mean pain score (\\(M = 2.89\\)) and the a minor pain (\\(\\mu_{hyp} = 2\\)) is meaningful in practical terms.\n\nSolution\nGiven \\(N = 120\\), \\(M \\approx 2.89\\), \\(SD \\approx 1.99\\), \\(\\mu_{hyp} = 2\\), the standard error \\(SE_M\\) is:\n\\[\nSE_M = \\frac{SD}{\\sqrt{N}} \\approx 0.18\n\\]\n\nfrom math import sqrt\n\n# Standard Error Mean\n# Note: I can use injury.sem() directly to get the result, \n# but I shall calculate by my own for this assignment. \n\ninjury_sem = injury.std(ddof=1) / sqrt(120)\nprint(f'Standard Error Mean: ', injury_sem)\n\nStandard Error Mean:  0.1821117309227567\n\n\nWith \\(SE_M \\approx 0.18\\), the \\(t\\) ratio is:\n\\[\nt = \\frac{M - \\mu_{hyp}}{SE_{M}} \\approx 4.90\n\\]\n\n# t statistic\ninjury_t = (injury.mean() - 2) / injury.sem()\nprint(f't: ', injury_t)\n\nt:  4.8962615540943375\n\n\nUnfortunately, I can’t calculate the \\(p\\)-value on hand, so in this part I’ll call scipy.stats.t for help. the degree of freedom (\\(df\\)) is:\n\\[\ndf = N - 1 = 120 - 1 = 119\n\\]\nWith \\(t \\approx 4.90\\) and \\(df = 119\\), then use survivor function to reach the \\(p\\)-value: \\[\np \\approx 3.11 \\times 10^{-6}\n\\]\n\nimport scipy.stats as stats\n\n# 119 is the degree of freedom; Two-sided times two\ninjury_p = stats.t.sf(injury_t, 119 ) * 2\n\nprint(f'p: ', injury_p)\n\np:  3.1051091723547962e-06\n\n\nThe \\(p\\)-value is much smaller than 0.001 (\\(p &lt; 0.001\\)), the null hypothesis should be rejected.\nI also did a sanity check with the ready-to-use function scipy.stats.ttest_1samp:\n\n# A san-check on my calculation result: \n\ninjury_ttest_1samp = stats.ttest_1samp(injury, 2, alternative='two-sided')\n\nprint(f't: ', injury_ttest_1samp.statistic, '\\n'\n      'df: ', injury_ttest_1samp.df, '\\n'\n      'p-value: ', injury_ttest_1samp.pvalue)\n\nt:  4.8962615540943375 \ndf:  119 \np-value:  3.1051091723547962e-06\n\n\nThe Cohen’s d value is:\n\\[\nd = \\frac{M - \\mu_{hyp}}{SD} = \\frac{t}{\\sqrt{N}} \\approx 0.45\n\\]\n\n# Effect Size d\n\ninjury_d = injury_t / sqrt(120)\nprint(f'Cohen\\'s d:', injury_d)\n\nCohen's d: 0.4469654834371283",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#q3-95-confidence-interval-for-the-mean-pain-score",
    "href": "session05-06/assignment.html#q3-95-confidence-interval-for-the-mean-pain-score",
    "title": "Laboratory 03",
    "section": "Q3: 95% confidence interval for the mean pain score",
    "text": "Q3: 95% confidence interval for the mean pain score\nAnswer\nBased on the sample of \\(N = 120\\) pain scores, with \\(M \\approx 2.89\\) and \\(SD \\approx 1.99\\), the 95% CI for pain scores is \\([2.53, 3.25]\\).\nSolution\nGiven \\(c = 1.96\\) for a 95% confidence interval and \\(SE_M \\approx 0.18\\) as calculated in the last section, a 95% confidence interval of the mean pain score is:\n\\[\n[M - c \\times SE_M, M + c \\times SE_M] \\approx [2.53, 3.25]\n\\]\n\n# CI for two-tailed t-statistics\ndef confidence_interval(alpha, mean, sem, df): \n    c = stats.t.interval(1 - alpha, df)[1]\n    ci_upper = mean + (c * sem)\n    ci_lower = mean - (c * sem)\n    print(f'CI (Lower): ', ci_lower)\n    print(f'CI (Upper): ', ci_upper)\n    return str(f'[{ci_lower}, {ci_upper}]')\n\ninjury_ci = confidence_interval(0.05, injury_mean, injury_sem, 119)\nprint(injury_ci)\n\nCI (Lower):  2.53106725077079\nCI (Upper):  3.252266082562543\n[2.53106725077079, 3.252266082562543]\n\n\n\n# And scipy.stats.t does the same thing. \nstats.t.interval(confidence=0.95, \n                 df=119, \n                 loc=injury_mean, \n                 scale=injury_sem)\n\n(2.53106725077079, 3.252266082562543)",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#q4-summarizing-the-findings",
    "href": "session05-06/assignment.html#q4-summarizing-the-findings",
    "title": "Laboratory 03",
    "section": "Q4: Summarizing the findings",
    "text": "Q4: Summarizing the findings\nAnswer\nA one‐sample \\(t\\)-test is conducted to reveal whether mean pain score for a sample of \\(N = 120\\) patients differed from the minor pain with a score of 2. For this example, \\(M = 2.89\\), \\(SD = 1.99\\) and \\(SE_M = 0.18\\). The 95% CI for \\(M\\) was \\([2.53, 3.25]\\). The result was \\(t(119) = 4.90\\), \\(p &lt; 0.001\\), two tailed. The effect size is \\(d = 0.45\\) by Cohen’s standards, which represents a medium effect. The difference between the sample mean (\\(M = 2.89\\)) and the score of minor pain (2) is statistically significant using \\(\\alpha = 0.05\\), two tailed.",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session05-06/assignment.html#q5-95-ci-for-switch-players-mean-pain-score",
    "href": "session05-06/assignment.html#q5-95-ci-for-switch-players-mean-pain-score",
    "title": "Laboratory 03",
    "section": "Q5: 95% CI for Switch players’ mean pain score",
    "text": "Q5: 95% CI for Switch players’ mean pain score\nAnswer\nThe 95% confidence interval for the mean pain score of those who played on a Nintendo Switch is \\([3.14, 4.33]\\).\nSolution\n\nCheck the structure of column switch then apply the filtering:\n\n\n# Check how many people get injured while playing\nprint(f'Variables in the column switch: \\n', switch['switch'].value_counts())\n# Filter out all switch players\ninjury_ns = switch[switch['switch'] == 'Playing switch']['injury']\n# And a sanity check\nprint(f'Filtered data: \\n', injury_ns.describe())\n\nVariables in the column switch: \n switch\nPlaying switch     60\nWatching switch    60\nName: count, dtype: int64\nFiltered data: \n count    60.000000\nmean      3.733333\nstd       2.313312\nmin       0.000000\n25%       2.000000\n50%       3.500000\n75%       5.000000\nmax      10.000000\nName: injury, dtype: float64\n\n\n\nCalculating the CI:\n\nGiven \\(N_{player} = 60\\), then the \\(df_{player} = N_{player} - 1 = 59\\),\nBased on the data we also have \\(M_{player} \\approx 3.73\\) and \\(SE_{M_{player}} \\approx 0.30\\)\nThe 95% confidence interval for the mean pain score of those who played on a Nintendo Switch is:\n\\[\n[M - c \\times SE_M, M + c \\times SE_M] \\approx [3.14, 4.33]\n\\]\n\ninjury_ns_mean = injury_ns.mean()\ninjury_ns_sem = injury_ns.sem()\ninjury_ns_dregf = len(injury_ns) - 1\n\nprint(f'Sample size: {len(injury_ns)}, \\n'\n      f'Degree of Freedom: {injury_ns_dregf},\\n'\n      f'Mean: {injury_ns_mean},\\n'\n      f'Standard Error: {injury_ns_sem}')\n\ninjury_ns_ci = confidence_interval(0.05, injury_ns_mean, injury_ns_sem, injury_ns_dregf)\nprint(f'\\nThe 95% CI for Switch players: \\n', injury_ns_ci)\n\nSample size: 60, \nDegree of Freedom: 59,\nMean: 3.7333333333333334,\nStandard Error: 0.29864729557842784\nCI (Lower):  3.1357414752023387\nCI (Upper):  4.3309251914643285\n\nThe 95% CI for Switch players: \n [3.1357414752023387, 4.3309251914643285]",
    "crumbs": [
      "Labs",
      "Session05 06",
      "Laboratory 03"
    ]
  },
  {
    "objectID": "session01/lecture-notes.html",
    "href": "session01/lecture-notes.html",
    "title": "Session 01: Introduction",
    "section": "",
    "text": "We didn’t discuss too much on statistics or quantitative methods.\nRather, we tried to import dataset into SPSS and played around with datasets below:\n\nmtcars: a built-in dataset in base-r, wrapped in an .xlsx file for some reason.\ntemphr10.sav: a dataset came from the textbook, looks like a series of weather data."
  },
  {
    "objectID": "session01/lecture-notes.html#summary",
    "href": "session01/lecture-notes.html#summary",
    "title": "Session 01: Introduction",
    "section": "",
    "text": "We didn’t discuss too much on statistics or quantitative methods.\nRather, we tried to import dataset into SPSS and played around with datasets below:\n\nmtcars: a built-in dataset in base-r, wrapped in an .xlsx file for some reason.\ntemphr10.sav: a dataset came from the textbook, looks like a series of weather data."
  },
  {
    "objectID": "session01/lecture-notes.html#implementations",
    "href": "session01/lecture-notes.html#implementations",
    "title": "Session 01: Introduction",
    "section": "Implementations",
    "text": "Implementations\nSince I use python for my daily work, the data were loaded to pandas.\n\n# Also, for descriptive statistics, pandas itself is enough for the job. \nimport pandas as pd\nimport seaborn as sns\n\n\n# Load the SPSS dataset called \"temphr10.sav\".\ntemphr10 = pd.read_spss('./datasets/temphr10.sav')\n# I converted the excel file to csv. \nmtcars = pd.read_csv('./datasets/mtcars.csv')\n\n\nIn SPSS: Measures of variables\n\n# What's in the variable view in SPSS\ntemphr10.columns\n\nIndex(['sex', 'hr', 'temp_Fahrenheit', 'temp_Celcius', 'Likert_rating'], dtype='object')\n\n\nIn SPSS: - Nominal: labels, e.g. races or gender / sex - Ordinal: ranking - Interval: ordinal, equally spaced, e.g. temperature, IQ - Ratio: interval + a true zero point, e.g. ago, wage, height, weight (it can be 0)\n\n\n\n\n\n\n\nLevel of Measurement\nValid Operations\n\n\n\n\nNominal\n\\(=\\), \\(\\neq\\)\n\n\nOrdinal\n\\(=\\), \\(\\neq\\), \\(&lt;\\), \\(&gt;\\)\n\n\nInterval\n\\(=\\), \\(\\neq\\), \\(&lt;\\), \\(&gt;\\), \\(+\\), \\(-\\)\n\n\nRatio\n\\(=\\), \\(\\neq\\), \\(&lt;\\), \\(&gt;\\), \\(+\\), \\(-\\), \\(\\times\\), \\(\\div\\)"
  },
  {
    "objectID": "session01/lecture-notes.html#descriptive-statistics",
    "href": "session01/lecture-notes.html#descriptive-statistics",
    "title": "Session 01: Introduction",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n# Ways to do descriptive things in pandas.\ntemphr10['sex'].describe()\n\ncount       10\nunique       2\ntop       Male\nfreq         7\nName: sex, dtype: object\n\n\n\ntemphr10['Likert_rating'].value_counts()\n\nLikert_rating\nNeutral or Don't Know    3\nAgree                    2\nDisagree                 2\nStrongly Disagree        2\nStrongly Agree           1\nName: count, dtype: int64\n\n\n\ntemphr10['hr'].value_counts()\n\nhr\n75.0    2\n70.0    1\n69.0    1\n71.0    1\n62.0    1\n74.0    1\n80.0    1\n73.0    1\n82.0    1\nName: count, dtype: int64"
  },
  {
    "objectID": "session09-10/assignment.html",
    "href": "session09-10/assignment.html",
    "title": "Laboratory 05",
    "section": "",
    "text": "For all typos on Lab 4 (e.g. 4.4 Q3): All fixed on the online version.",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#a-peek-on-the-dataset-as-always",
    "href": "session09-10/assignment.html#a-peek-on-the-dataset-as-always",
    "title": "Laboratory 05",
    "section": "A peek on the dataset as always",
    "text": "A peek on the dataset as always\n\nimport pandas as pd\n\ntea = pd.read_spss('./datasets/tea.sav')\nprint(tea.describe())\n\n              tea     cog_fun\ncount  716.000000  716.000000\nmean     3.032123   50.611732\nstd      1.669245    9.882814\nmin      0.000000   24.000000\n25%      2.000000   44.000000\n50%      3.000000   51.000000\n75%      4.000000   57.000000\nmax      8.000000   80.000000",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#q1-the-bivariate-regression-equation",
    "href": "session09-10/assignment.html#q1-the-bivariate-regression-equation",
    "title": "Laboratory 05",
    "section": "Q1: The bivariate regression equation",
    "text": "Q1: The bivariate regression equation\nAnswer\nGiven the independent variable \\(X\\), the predicted \\(\\hat Y_{cog\\_fun}\\) 2 is:\n\\[\n\\hat Y_{cog\\_fun} = \\beta_0 + \\beta_1X\n\\]",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#q2-whether-bi-variate-regression-is-a-good-choice-for-the-data",
    "href": "session09-10/assignment.html#q2-whether-bi-variate-regression-is-a-good-choice-for-the-data",
    "title": "Laboratory 05",
    "section": "Q2: Whether bi-variate regression is a good choice for the data",
    "text": "Q2: Whether bi-variate regression is a good choice for the data\nAnswer\nThe bi-variate regression may not be a strong model for this data for:\n\nThe Pearson’s Correlation: The Pearson’s correlation coefficient (\\(r \\approx 0.0777\\)) indicates a relatively weak positive relationship between tea consumption and cognitive function. The \\(p\\)-value (\\(p \\approx 0.0378\\)) is below 0.05, suggesting that the weak correlation is statistically significant.\nLinearity and Distribution: The scatter plot in Figure 1 shows that a weak and flat linear relationship between tea consumption and cognitive function, which aligns with the low Pearson’s \\(r\\). While the histograms in Figure 2 show that the cognitive function scores are approximately normally distributed.\nHowever, the sample size (\\(N = 716\\)) is relatively large, which may be beneficial for conducting a regression.\n\nBased on this weak linear trend, although bi-variate regression could be employed as they meet the assumption of a linear regression, the model may not explain much variability in cognitive function. Other factors like age, education background even SES may (and must) influence cognitive function. For sake of this assignment, I will assume that other variables either do not vary significantly across participants or are not measured.\nSolution\n\nimport scipy.stats as stats\ntea_pearsonr, tea_pvalue = stats.pearsonr(tea.tea, tea.cog_fun)\nprint(f'Pearson\\'s r: {tea_pearsonr}\\n'\n      f'p-value: {tea_pvalue}')\n\nPearson's r: 0.07765252417388956\np-value: 0.03776954868447832\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntea_scatter = sns.lmplot(tea, x='tea', y='cog_fun')\ntea_scatter.fig.suptitle('Scatter plot on tea v. cog_fun')\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Scatter Plot on tea v. cog_fun\n\n\n\n\n\n\nfig, ax =plt.subplots(1,2)\nsns.histplot(tea, x='tea', kde=True, bins=8, ax=ax[0])\nsns.histplot(tea, x='cog_fun', kde=True, ax=ax[1])\nax[0].set_title('Tea')\nax[1].set_title('Cognitive Function')\nfig.suptitle('Histograms on variables: Tea and Cognitive Function')\n\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Distribution of cups of tea and cognitive function scores",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#q3-bivariate-regression-analysis",
    "href": "session09-10/assignment.html#q3-bivariate-regression-analysis",
    "title": "Laboratory 05",
    "section": "Q3: Bivariate regression analysis",
    "text": "Q3: Bivariate regression analysis\nAnswer\nIntercept: \\(\\beta_0 \\approx 49.2177\\), suggests that when a respondent drinks 0 cups of tea per day, the expected cognitive function score is 49.2177.\nSlope: \\(\\beta_1 \\approx 0.4597\\), indicates a positive relationship between tea consumption and cognitive function. For each additional cup of tea consumed per day, the cognitive function score increases by approximately 0.4597 points.\nSolution\n\ndef print_linregress(x, y): \n      b1, b0, r, p, stderr =  stats.linregress(x=x, y=y)\n      print(f'Intercept: {b0}')\n      print(f'Slope: {b1}')\n      print(f'R-squared: {r**2}')\n      print(f'P-value: {p}')\n      print(f'Standard error: {stderr}')\n\nprint_linregress(tea['tea'], tea['cog_fun'])\n\nIntercept: 49.217731459509174\nSlope: 0.45974402348753085\nR-squared: 0.006029914510576479\nP-value: 0.03776954868447834\nStandard error: 0.22090119534570854\n\n\nIn the world of python, scipy.stats works fine for applying basic linear models, however, statsmodels gives you this pretty damn cool all-in-one summary output as shown below:\n\nimport statsmodels.api as sm\n\ndef bivariate_regression(x, y): \n    X = sm.add_constant(x)\n    model = sm.OLS(y, X).fit()\n    summary = model.summary()\n    print(summary)\n    return model\n\ntea_model = bivariate_regression(tea['tea'], tea['cog_fun'])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                cog_fun   R-squared:                       0.006\nModel:                            OLS   Adj. R-squared:                  0.005\nMethod:                 Least Squares   F-statistic:                     4.331\nDate:                Mon, 28 Oct 2024   Prob (F-statistic):             0.0378\nTime:                        19:52:27   Log-Likelihood:                -2653.5\nNo. Observations:                 716   AIC:                             5311.\nDf Residuals:                     714   BIC:                             5320.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         49.2177      0.764     64.382      0.000      47.717      50.719\ntea            0.4597      0.221      2.081      0.038       0.026       0.893\n==============================================================================\nOmnibus:                        0.140   Durbin-Watson:                   1.912\nProb(Omnibus):                  0.932   Jarque-Bera (JB):                0.229\nSkew:                          -0.009   Prob(JB):                        0.892\nKurtosis:                       2.914   Cond. No.                         7.65\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#q4-the-slope-from-the-standardized-regression",
    "href": "session09-10/assignment.html#q4-the-slope-from-the-standardized-regression",
    "title": "Laboratory 05",
    "section": "Q4: The slope from the standardized regression",
    "text": "Q4: The slope from the standardized regression\nAnswer\nQ4a: Interpret the slope from the standardized regression.\n\\[\\beta^* \\approx 0.07765\\]\nThe standardized slope is approximately 0.07765, means that for every one standard deviation increase in tea consumption, the cognitive function score increases by 0.07765 standard deviations.\nQ4b: From which regression (raw or standardized) is the slope easier to interpret and understand?\nIt depends.\nThe raw slope is easy to interpret in the context of the actual units of measurement as it represents the change in the dependent variable (cog_fun) for a one-unit change in the independent variable (tea).\nThe standardized slope makes comparing effects across different variables or studies much easier as it tells us how many standard deviations the dependent variable changes for a one standard deviation change in the independent variable.\nSolution\nBoth the independent (tea) and dependent variables (cog_fun) are transformed into z-scores, then:\n\n# Preparing standardized\ntea['tea_std'] = tea['tea'].apply(lambda x: (x - tea['tea'].mean()) / tea['tea'].std())\ntea['cog_fun_std'] = tea['cog_fun'].apply(lambda x: (x - tea['cog_fun'].mean()) / tea['cog_fun'].std())\n\n# Intercept and slope of standardized regression\nprint_linregress(tea['tea_std'], tea['cog_fun_std'])\n\nIntercept: -1.2880252063838454e-16\nSlope: 0.07765252417388924\nR-squared: 0.006029914510576491\nP-value: 0.0377695486844781\nStandard error: 0.03731105688226283\n\n\n\n# And the pretty damn detailed OLS summary\nbivariate_regression(tea['tea_std'], tea['cog_fun_std'])\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:            cog_fun_std   R-squared:                       0.006\nModel:                            OLS   Adj. R-squared:                  0.005\nMethod:                 Least Squares   F-statistic:                     4.331\nDate:                Mon, 28 Oct 2024   Prob (F-statistic):             0.0378\nTime:                        19:52:27   Log-Likelihood:                -1013.3\nNo. Observations:                 716   AIC:                             2031.\nDf Residuals:                     714   BIC:                             2040.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst      -6.765e-17      0.037  -1.81e-15      1.000      -0.073       0.073\ntea_std        0.0777      0.037      2.081      0.038       0.004       0.151\n==============================================================================\nOmnibus:                        0.140   Durbin-Watson:                   1.912\nProb(Omnibus):                  0.932   Jarque-Bera (JB):                0.229\nSkew:                          -0.009   Prob(JB):                        0.892\nKurtosis:                       2.914   Cond. No.                         1.00\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n&lt;statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ff15145ae90&gt;",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#q5-cognitive-function-score-if-drink-3-cups-of-tea-per-day",
    "href": "session09-10/assignment.html#q5-cognitive-function-score-if-drink-3-cups-of-tea-per-day",
    "title": "Laboratory 05",
    "section": "Q5: Cognitive function score if drink 3 cups of tea per day",
    "text": "Q5: Cognitive function score if drink 3 cups of tea per day\nAnswer\nGiven \\(X = 3\\), the cognitive function score for a respondent who drinks 3 cups of tea per day is:\n\\[\\hat Y = \\beta_0 + \\beta_1 \\times 3 \\approx 50.5970\\]\nHowever, the prediction might not be very close to the true score. Although the slope is statistically significant (with a \\(p\\)-value of \\(p = 0.038\\)), the low \\(R\\)-squared (\\(R^2 \\approx 0.006\\)) indicates that statistically, there’s only 0.6% of the variance in cognitive function scores is explained by tea consumption.\nSolution\n\ncog_fun_after_3_cups = tea_model.predict([1.0, 3]) # The 1.0 here is the constant required by statsmodels\nprint(f'Cognitive function score if drink 3 cups of tea per day: \\n'\n      f'{cog_fun_after_3_cups[0]}')\n\nCognitive function score if drink 3 cups of tea per day: \n50.59696352997175",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#q6-tea-predicts-cognitive-function",
    "href": "session09-10/assignment.html#q6-tea-predicts-cognitive-function",
    "title": "Laboratory 05",
    "section": "Q6: Tea predicts cognitive function?",
    "text": "Q6: Tea predicts cognitive function?\nAnswers\nThe \\(p\\)-value for the slope is:\n\\[p \\approx 0.0378\\]\nSince the \\(p\\)-value is less than 0.05 (\\(p &lt; 0.05\\)), we can reject the null hypothesis (\\(H_0\\)). This suggests that there is evidence of an association between tea consumption and cognitive function scores.\nHowever, the low \\(R\\)-squared value (\\(R^2 \\approx 0.006\\)) indicates that tea consumption explains only a tiny fraction of the variability in cognitive function.\nWhile the results suggest a statistically significant but weak association between tea consumption and cognitive function, they do not provide evidence of a causal effect. Besides, if simply drinking more tea could raise cognitive scores, we might think about advising children to drink tea to guarantee a bright future! That said, those less inclined to drink tea might still end up in schools of education.\nSolution\nHypothesis:\n\\(H_0\\): The slope (\\(\\beta_1\\)) is zero, meaning tea consumption does not predict cognitive function.\n\\(H_1\\): The slope (\\(\\beta_1\\)) is not zero, meaning tea consumption does predict cognitive function.\n\ntea_p_value = tea_model.pvalues['tea']\n\ndef test_hypothesis(p_value, alpha=0.05): \n    print(f'Given the p-value={p_value} and alpha={alpha}:')\n    if p_value &lt; alpha:\n        print(\"Reject the null hypothesis.\")\n    else:\n        print(\"Fail to reject the null hypothesis.\")\n\ntest_hypothesis(tea_p_value, alpha=0.05)\n\nGiven the p-value=0.03776954868447742 and alpha=0.05:\nReject the null hypothesis.",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session09-10/assignment.html#footnotes",
    "href": "session09-10/assignment.html#footnotes",
    "title": "Laboratory 05",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOh wait, could this kind of torture be approved by the IRB?↩︎\nAdd \\(\\epsilon\\) if consider the error term.↩︎",
    "crumbs": [
      "Labs",
      "Session09 10",
      "Laboratory 05"
    ]
  },
  {
    "objectID": "session07-08/assignment.html",
    "href": "session07-08/assignment.html",
    "title": "Laboratory 04",
    "section": "",
    "text": "Again, thanks for your feedback on last assignment. Here’s my response:\n\n\nAbsolutely! The awesome scipy library does so much more than a TI-84. The scipy.stats.t module covers all aspects of the \\(t\\)-test workflow, and the interval method provides the confidence interval in no time:\nstats.t.interval(confidence=0.95, \n                 df=119, \n                 loc=injury_mean, \n                 scale=injury_sem)\nWhich outputs:\n(2.53106725077079, 3.252266082562543)\nHumans make mistakes sometimes, so calling the function not only saves time but also helps avoid the chance of errors. As you explained the equations in the lecture and demonstrated the calculations, I think it’s best to follow your approach for the math as well.\nI do this only when I’m doing assignments and just for fun. (In most cases) I never trust myself and my knowledge so I never do these dangerous thing to the real world data. :P\n\n\n\nGot it! This part was refined and updated on the web doc.\n\n\n\n{\\huge NOOOOOOOOOOOO!}\nI believe my fellow classmates and I suffered enough from the traps in the quizzes.\n\n\n\nMy lord jumped on my desk for a warm greeting:\n\n\n\nThe Lord",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#on-4.4-q3-function-for-calculating-the-ci",
    "href": "session07-08/assignment.html#on-4.4-q3-function-for-calculating-the-ci",
    "title": "Laboratory 04",
    "section": "",
    "text": "Absolutely! The awesome scipy library does so much more than a TI-84. The scipy.stats.t module covers all aspects of the \\(t\\)-test workflow, and the interval method provides the confidence interval in no time:\nstats.t.interval(confidence=0.95, \n                 df=119, \n                 loc=injury_mean, \n                 scale=injury_sem)\nWhich outputs:\n(2.53106725077079, 3.252266082562543)\nHumans make mistakes sometimes, so calling the function not only saves time but also helps avoid the chance of errors. As you explained the equations in the lecture and demonstrated the calculations, I think it’s best to follow your approach for the math as well.\nI do this only when I’m doing assignments and just for fun. (In most cases) I never trust myself and my knowledge so I never do these dangerous thing to the real world data. :P",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#on-4.5-q4-p-value-in-my-report",
    "href": "session07-08/assignment.html#on-4.5-q4-p-value-in-my-report",
    "title": "Laboratory 04",
    "section": "",
    "text": "Got it! This part was refined and updated on the web doc.",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#on-4.2.2-q1b-trap-in-the-future",
    "href": "session07-08/assignment.html#on-4.2.2-q1b-trap-in-the-future",
    "title": "Laboratory 04",
    "section": "",
    "text": "{\\huge NOOOOOOOOOOOO!}\nI believe my fellow classmates and I suffered enough from the traps in the quizzes.",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#while-i-was-writing-this-part",
    "href": "session07-08/assignment.html#while-i-was-writing-this-part",
    "title": "Laboratory 04",
    "section": "",
    "text": "My lord jumped on my desk for a warm greeting:\n\n\n\nThe Lord",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#a-quick-dataset-check-up",
    "href": "session07-08/assignment.html#a-quick-dataset-check-up",
    "title": "Laboratory 04",
    "section": "A quick dataset check-up",
    "text": "A quick dataset check-up\n\nimport pandas as pd\ncatterplot_df = pd.read_spss('./datasets/catterplot.sav')\n\nprint(f'Description on the dataset: \\n',catterplot_df.describe())\n\nDescription on the dataset: \n        dinner_time       meow\ncount    78.000000  78.000000\nmean      9.865385   8.217949\nstd       6.068159   3.747371\nmin       1.000000   2.000000\n25%       5.000000   5.000000\n50%       9.000000   8.000000\n75%      14.000000  12.000000\nmax      24.000000  15.000000",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#q1-data-distribution",
    "href": "session07-08/assignment.html#q1-data-distribution",
    "title": "Laboratory 04",
    "section": "Q1: Data distribution",
    "text": "Q1: Data distribution\nAnswer\n\n\n\nTable 1: Mean, mode and median\n\n\n\n\n\nMeasurement\nValue\n\n\n\n\nMean\n9.87\n\n\nMedian\n9.00\n\n\nMode\n5.00\n\n\n\n\n\n\nTable 1 lists measurements on the central tendency of this data, the mean (9.87) is slightly higher than the median (9.00) but much higher than the mode (5.00). The distribution is right-skewed as shown by Figure 1, and a longer tail is found on the right side. There is a clear peak around 5, which indicates most cats in this sample were fed with in 5 hours.\nSolution\nTo describe the distribution of the dataset, I will again use the mean, median, and mode to assess central tendency, along with a histogram to visualize the distribution.\n\ndef centrality(df): \n    mean = df.mean()\n    median = df.median()\n    mode = df.mode()[0]\n    return mean, median, mode\n\ncats_mean, cats_median, cats_mode = centrality(catterplot_df['dinner_time'])\nprint(f'Central Tendency: \\n'\n      f'Mean: {cats_mean}, \\n'\n      f'Median: {cats_median} \\n' \n      f'Mode: {cats_mode}')\n\nCentral Tendency: \nMean: 9.865384615384615, \nMedian: 9.0 \nMode: 5.0\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Create a histogram to reveal the distribution. \ndinner_time_hist = sns.histplot(catterplot_df, x='dinner_time', kde=True, bins=23)\n# Set titles and labels for x- and y- axis\ndinner_time_hist.set_title('Distribution of the time since last feeding a cat')\ndinner_time_hist.set_xlabel('last_feed: Time elapsed since the last feed')\ndinner_time_hist.set_ylabel('Frequency')\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\nFigure 1: Distribution of Dinner Time",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#q2-the-scatterplot",
    "href": "session07-08/assignment.html#q2-the-scatterplot",
    "title": "Laboratory 04",
    "section": "Q2: The (s)catterplot",
    "text": "Q2: The (s)catterplot\nAnswer\nThe scatter plot in Figure 2 shows that the data, at least, forms a perfect shape of a cat! However, the scattered points and the gentle slope of the LOESS curve suggest that there is not a strong linear relationship between the time since the last feeding (dinner_time) and the loudness of purring (meow). The scatter plot also indicates a weak correlation between these two variables.\nSolution\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncatterplot = sns.regplot(catterplot_df, x='dinner_time', y='meow', fit_reg=True, lowess=True)\ncatterplot.set_title('The (S)catterplot')\ncatterplot.set_xlabel('last_feed: Time elapsed since the last feed')\ncatterplot.set_ylabel('meow: Loudness of Purring')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: The (s)catterplot on meow againt dinnertime.",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#q3-the-pearsons-r-of-dinner_time-v.-meow",
    "href": "session07-08/assignment.html#q3-the-pearsons-r-of-dinner_time-v.-meow",
    "title": "Laboratory 04",
    "section": "Q3: The Pearson’s \\(r\\) of dinner_time v. meow",
    "text": "Q3: The Pearson’s \\(r\\) of dinner_time v. meow\nAnswer\n\\(r \\approx -6.12 \\times 10^{-3}\\)\nThe pearson’s correlation coefficient \\(r \\approx -6.12 \\times 10^{-3}\\) is very close to zero, suggesting there may be no linear relationship between dinner_time and meow in this dataset. However, it’s important to note that the data lacks further details, particularly regarding the sampling procedure. Specifically, we don’t know if the data represents multiple observations of a single cat (in which case it would be a single case) or if it was collected from several different cats (in which case the sample size and whether the sample is similar to population of interest needs to be justified). Although the correlation is statistically weak, we cannot definitively conclude that there is no relationship between the given variables.1\nIn conclusion, the pearson’s \\(r\\) suggests there is no strong linear relationship between the time since last feed and the volume of cats’ purring. That’s all I can say.\nSolution\nTo calculate the pearson’s \\(r\\), I use the formula2:\n\\[r = \\frac{\\sum{Z_XZ_Y}}{N-1}\\]\n\nimport scipy.stats as stats\n\npearson_r_results_cats = stats.pearsonr(catterplot_df.meow, catterplot_df.dinner_time)\n\npearson_r_cats = pearson_r_results_cats[0]\np_cats = pearson_r_results_cats[1]\n\nprint(f'Correlation Coefficient\\n'\n      f'Pearson-r: {pearson_r_cats}\\n'\n      f'p-value: {pearson_r_results_cats[1]}')\n\nCorrelation Coefficient\nPearson-r: -0.006117549224989473\np-value: 0.9576068830096687",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#q4-test-h_0-at-the-5-sig-level",
    "href": "session07-08/assignment.html#q4-test-h_0-at-the-5-sig-level",
    "title": "Laboratory 04",
    "section": "Q4: Test \\(H_0\\) at the 5% sig-level",
    "text": "Q4: Test \\(H_0\\) at the 5% sig-level\nAnswer\n\\(p \\approx 0.9576\\)\nThe \\(p\\)-value is much greater than the significance level \\(\\alpha = 0.05\\), therefore, we fail to reject the null hypothesis (\\(H_0\\)) that dinner_time and meow are not linearly related. This means that there is insufficient evidence to suggest a linear relationship between dinner_time and meow at the 5% significance level, the sample does not indicate a significant linear correlation between the two variables.\nSolution\nThe null hypothesis (\\(H_0\\)) and the alternative hypothesis (\\(H_1\\)) are formulated as follows:\n\n\\(H_0:\\rho = 0\\): no linear relationship.\n\\(H_1:\\rho \\neq 0\\): linear relationship exists.\n\nGiven \\(r \\approx -0.0061\\), \\(p \\approx 0.9576\\) and \\(\\alpha = 0.05\\):\n\ndef test_hypothesis(p_value, alpha=0.05): \n    print(f'Given the p-value={p_value} and alpha={alpha}:')\n    if p_value &lt; alpha:\n        print(\"Reject the null hypothesis: \\nThere is evidence of a linear relationship.\")\n    else:\n        print(\"Fail to reject the null hypothesis: \\nNo significant linear relationship.\")\n\ntest_hypothesis(p_cats, alpha=0.05)\n\nGiven the p-value=0.9576068830096687 and alpha=0.05:\nFail to reject the null hypothesis: \nNo significant linear relationship.",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#q5-95-ci-of-rho",
    "href": "session07-08/assignment.html#q5-95-ci-of-rho",
    "title": "Laboratory 04",
    "section": "Q5: 95% CI of \\(\\rho\\)",
    "text": "Q5: 95% CI of \\(\\rho\\)\nAnswer\nThe 95% confidence interval for the population correlation coefficient \\(\\rho\\) is approximately: \\[[-0.2283, 0.2167]\\]\nSolution\nGiven \\(N = 78\\), \\(r = -6.12 \\times 10^{-3}\\), the 95% confidence interval for the population correlation coefficient \\(\\rho\\) is then calculated based on the Fisher \\(z\\)-transformation, steps of calculation is listed below and I wrapped the calculation procedure in a single function:\n\\(z\\)-transformation\n\\[\nz = \\frac{1}{2}ln(\\frac{1+r}{1-r}) = artanh(r)\n\\]\nStandard error of \\(z\\)\n\\[\nSE_z = \\frac{1}{\\sqrt{N-3}}\n\\]\nCI in the \\(z\\)-scale\n\\[\nz_{lower} = z - c \\times SE_z\n\\]\n\\[\nz_{upper} = z - c \\times SE_z\n\\]\nInverse Fisher transformation\n\\[\nr = \\frac{exp(2z)-1}{exp(2z)+1} = tanh(z)\n\\]\nFinally we get\n\\[\nr_{lower} = tanh(z_{lower})\n\\] \\[\nr_{upper} = tanh(z_{upper})\n\\]\n\nfrom math import sqrt, tanh, atanh\n\nn_cats = len(catterplot_df)\n\ndef ci_rho(n, pearson_r, alpha=0.05):\n    # z-transformation\n    z = atanh(pearson_r)\n    # SEM of z\n    se_z = 1 / sqrt(n - 3)\n    # CI in the z score in the 95% level.  \n    c = stats.norm.ppf(1 - alpha / 2)\n    z_lower = z - c * se_z\n    z_upper = z + c * se_z\n    # Inverse Fisher transformation\n    r_lower = tanh(z_lower)\n    r_upper = tanh(z_upper)\n    return r_lower, r_upper\n\nr_lower_cats, r_upper_cats = ci_rho(n_cats, pearson_r_cats)\n\nprint(f'0.95 Confidence Intervals of rho: \\n'\n      f'Lower Limit: {r_lower_cats} \\n'\n      f'Upper Limit: {r_upper_cats}')\n\n0.95 Confidence Intervals of rho: \nLower Limit: -0.22833745967041358 \nUpper Limit: 0.21670822097748485\n\n\n\n# Or just call the function: \n\n# Ths pearson-r results was calculated in Q3, and stored in pearson_r_results_cats\n# Just apply the confidence_interval method on the pearson_r_results_cats, and bingo! \n\nci_rho_cats = pearson_r_results_cats.confidence_interval(confidence_level=0.95) # It does the Fisher transformation by default\n\nprint(f'0.95 Confidence Intervals of rho: \\n'\n      f'Lower Limit: {ci_rho_cats[0]} \\n'\n      f'Upper Limit: {ci_rho_cats[1]}')\n\n0.95 Confidence Intervals of rho: \nLower Limit: -0.22833745967041358 \nUpper Limit: 0.21670822097748485",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session07-08/assignment.html#footnotes",
    "href": "session07-08/assignment.html#footnotes",
    "title": "Laboratory 04",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs a devoted servant to cat, I must say that, in my experience, our cat never purrs after a meal. So, there’s definitely no relationship between those variables in my case. Staying silent seems to be his strategy for navigating this wild and dangerous world.↩︎\nI just put the formula here for … nothing? I didn’t actually calculated on hand this time. :P↩︎\nAn easter egg here, something can’t be compiled by {}.↩︎",
    "crumbs": [
      "Labs",
      "Session07 08",
      "Laboratory 04"
    ]
  },
  {
    "objectID": "session03/assignment-r.html",
    "href": "session03/assignment-r.html",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "What do 1, 2, and 3 mean in variable “gender”? What are their percentages in the sample?\nDraw a barplot of “gender” whose 𝑦-axis represents the percentage of each group.\nDraw a histogram of “day_1”. Is there anything wrong with this variable?\nHow many cases have missing values for “day_2”?\n(Extra credit) Exclude the outlier.\n\nbase-r version 4.3.3\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# Using `foreign` is truly a pain for handling SPSS data, I'll use heaven instead. \ndownload_festival &lt;- haven::read_sav('./datasets/download.sav')\n\n\n\n\nprint('A summary and structure of this dataset')\n\n[1] \"A summary and structure of this dataset\"\n\nsummary(download_festival)\n\n   ticket_no        gender          day_1            day_2       \n Min.   :2111   Min.   :1.000   Min.   : 0.020   Min.   :0.0000  \n 1st Qu.:3096   1st Qu.:1.000   1st Qu.: 1.312   1st Qu.:0.4100  \n Median :3620   Median :2.000   Median : 1.790   Median :0.7900  \n Mean   :3616   Mean   :1.769   Mean   : 1.793   Mean   :0.9609  \n 3rd Qu.:4155   3rd Qu.:2.000   3rd Qu.: 2.230   3rd Qu.:1.3500  \n Max.   :4765   Max.   :3.000   Max.   :20.020   Max.   :3.4400  \n                                                 NA's   :546     \n     day_3       \n Min.   :0.0200  \n 1st Qu.:0.4400  \n Median :0.7600  \n Mean   :0.9765  \n 3rd Qu.:1.5250  \n Max.   :3.4100  \n NA's   :687     \n\nstr(download_festival)\n\ntibble [810 × 5] (S3: tbl_df/tbl/data.frame)\n $ ticket_no: num [1:810] 2111 2229 2338 2384 2401 ...\n  ..- attr(*, \"label\")= chr \"Ticket number\"\n  ..- attr(*, \"format.spss\")= chr \"F4.0\"\n $ gender   : dbl+lbl [1:810] 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ...\n   ..@ label      : chr \"Gender of concert goer\"\n   ..@ format.spss: chr \"F8.0\"\n   ..@ labels     : Named num [1:3] 1 2 3\n   .. ..- attr(*, \"names\")= chr [1:3] \"Male\" \"Female\" \"Non-binary\"\n $ day_1    : num [1:810] 2.64 0.97 0.84 3.03 0.88 0.85 1.56 3.02 2.29 1.11 ...\n  ..- attr(*, \"label\")= chr \"Hygiene (day 1 of download festival)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ day_2    : num [1:810] 1.35 1.41 NA NA 0.08 NA NA NA NA 0.44 ...\n  ..- attr(*, \"label\")= chr \"Hygiene (day 2 of download festival)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ day_3    : num [1:810] 1.61 0.29 NA NA NA NA NA NA NA 0.55 ...\n  ..- attr(*, \"label\")= chr \"Hygiene (day 3 of download festival)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n\n\n\n\n\n\nWhat do 1, 2, and 3 mean in variable “gender”?\n\n\nprint('Q1a: What do 1, 2, and 3 mean in variable “gender”?')\n\n[1] \"Q1a: What do 1, 2, and 3 mean in variable “gender”?\"\n\ndownload_festival$gender %&gt;%\n  attr('labels')\n\n      Male     Female Non-binary \n         1          2          3 \n\n\n\n\n\n\nWhat are their percentages in the sample?\n\n\nprint('Q1b: What are their percentages in the sample?')\n\n[1] \"Q1b: What are their percentages in the sample?\"\n\ngender_percentage &lt;- download_festival %&gt;%\n  group_by(gender) %&gt;%\n  summarise(Percentage = n() / nrow(download_festival) * 100)\nprint(gender_percentage)\n\n# A tibble: 3 × 2\n  gender         Percentage\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [Male]             34.2\n2 2 [Female]           54.7\n3 3 [Non-binary]       11.1\n\n\n\n\n\n\nDraw a barplot of gender whose \\(y\\)-axis represents the percentage of each group.\n\n\ndownload_festival$gender &lt;- factor(download_festival$gender, levels = c(1,2,3), labels = c('Male', 'Female', 'Non-binary'))\n\nggplot(download_festival, aes(x=gender)) + \n  geom_bar() + \n  xlab('Gender') + \n  ylab('Percentage') + \n  scale_y_continuous(labels = scales::percent) + \n  ggtitle('Percentage of Gender')\n\n\n\n\n\n\n\n\n\n\n\n\nDraw a histogram of day_1. Is there anything wrong with this variable?\n\n\nprint('Draw a histogram of “day_1”. Is there anything wrong with this variable?')\n\n[1] \"Draw a histogram of “day_1”. Is there anything wrong with this variable?\"\n\nggplot(download_festival, aes(x=day_1)) + \n  geom_histogram(color = 1, bins=15) + \n  xlab('Day 1') + \n  ylab('Frequency / Count') + \n  ggtitle('Histogram of hygiene score in the 1st Day (Raw data)')\n\n\n\n\n\n\n\n  print('An outlier found.')\n\n[1] \"An outlier found.\"\n\n\n\n\n\n\nHow many cases have missing values for day_2?\n\n\nprint('How many cases have missing values for “day_2”?')\n\n[1] \"How many cases have missing values for “day_2”?\"\n\nsummary(download_festival)\n\n   ticket_no           gender        day_1            day_2       \n Min.   :2111   Male      :277   Min.   : 0.020   Min.   :0.0000  \n 1st Qu.:3096   Female    :443   1st Qu.: 1.312   1st Qu.:0.4100  \n Median :3620   Non-binary: 90   Median : 1.790   Median :0.7900  \n Mean   :3616                    Mean   : 1.793   Mean   :0.9609  \n 3rd Qu.:4155                    3rd Qu.: 2.230   3rd Qu.:1.3500  \n Max.   :4765                    Max.   :20.020   Max.   :3.4400  \n                                                  NA's   :546     \n     day_3       \n Min.   :0.0200  \n 1st Qu.:0.4400  \n Median :0.7600  \n Mean   :0.9765  \n 3rd Qu.:1.5250  \n Max.   :3.4100  \n NA's   :687     \n\nprint('Answer\\'s on the 7th row, 546 cases missing.')\n\n[1] \"Answer's on the 7th row, 546 cases missing.\"\n\n\n\n\n\n\nExclude the outlier.\n\n\nprint('Exclude Ms. Laundry Pod.')\n\n[1] \"Exclude Ms. Laundry Pod.\"\n\ndownload_festival_no_outliers &lt;- download_festival[download_festival$day_1 &lt;= 5, ]\nggplot(download_festival_no_outliers, aes(x=day_1)) + \n  geom_histogram(color = 1, bins=15) + \n  labs(title='Histogram of Hygiene Scores on the 1st Day', \n        subtitle='Outlier removed') + \n  xlab('Day 1') + \n  ylab('Frequency / Count')\n\n\n\n\n\n\n\n\n[EOF]",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#first-peek",
    "href": "session03/assignment-r.html#first-peek",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "print('A summary and structure of this dataset')\n\n[1] \"A summary and structure of this dataset\"\n\nsummary(download_festival)\n\n   ticket_no        gender          day_1            day_2       \n Min.   :2111   Min.   :1.000   Min.   : 0.020   Min.   :0.0000  \n 1st Qu.:3096   1st Qu.:1.000   1st Qu.: 1.312   1st Qu.:0.4100  \n Median :3620   Median :2.000   Median : 1.790   Median :0.7900  \n Mean   :3616   Mean   :1.769   Mean   : 1.793   Mean   :0.9609  \n 3rd Qu.:4155   3rd Qu.:2.000   3rd Qu.: 2.230   3rd Qu.:1.3500  \n Max.   :4765   Max.   :3.000   Max.   :20.020   Max.   :3.4400  \n                                                 NA's   :546     \n     day_3       \n Min.   :0.0200  \n 1st Qu.:0.4400  \n Median :0.7600  \n Mean   :0.9765  \n 3rd Qu.:1.5250  \n Max.   :3.4100  \n NA's   :687     \n\nstr(download_festival)\n\ntibble [810 × 5] (S3: tbl_df/tbl/data.frame)\n $ ticket_no: num [1:810] 2111 2229 2338 2384 2401 ...\n  ..- attr(*, \"label\")= chr \"Ticket number\"\n  ..- attr(*, \"format.spss\")= chr \"F4.0\"\n $ gender   : dbl+lbl [1:810] 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ...\n   ..@ label      : chr \"Gender of concert goer\"\n   ..@ format.spss: chr \"F8.0\"\n   ..@ labels     : Named num [1:3] 1 2 3\n   .. ..- attr(*, \"names\")= chr [1:3] \"Male\" \"Female\" \"Non-binary\"\n $ day_1    : num [1:810] 2.64 0.97 0.84 3.03 0.88 0.85 1.56 3.02 2.29 1.11 ...\n  ..- attr(*, \"label\")= chr \"Hygiene (day 1 of download festival)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ day_2    : num [1:810] 1.35 1.41 NA NA 0.08 NA NA NA NA 0.44 ...\n  ..- attr(*, \"label\")= chr \"Hygiene (day 2 of download festival)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"\n $ day_3    : num [1:810] 1.61 0.29 NA NA NA NA NA NA NA 0.55 ...\n  ..- attr(*, \"label\")= chr \"Hygiene (day 3 of download festival)\"\n  ..- attr(*, \"format.spss\")= chr \"F8.2\"",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#q1a",
    "href": "session03/assignment-r.html#q1a",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "What do 1, 2, and 3 mean in variable “gender”?\n\n\nprint('Q1a: What do 1, 2, and 3 mean in variable “gender”?')\n\n[1] \"Q1a: What do 1, 2, and 3 mean in variable “gender”?\"\n\ndownload_festival$gender %&gt;%\n  attr('labels')\n\n      Male     Female Non-binary \n         1          2          3",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#q1b",
    "href": "session03/assignment-r.html#q1b",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "What are their percentages in the sample?\n\n\nprint('Q1b: What are their percentages in the sample?')\n\n[1] \"Q1b: What are their percentages in the sample?\"\n\ngender_percentage &lt;- download_festival %&gt;%\n  group_by(gender) %&gt;%\n  summarise(Percentage = n() / nrow(download_festival) * 100)\nprint(gender_percentage)\n\n# A tibble: 3 × 2\n  gender         Percentage\n  &lt;dbl+lbl&gt;           &lt;dbl&gt;\n1 1 [Male]             34.2\n2 2 [Female]           54.7\n3 3 [Non-binary]       11.1",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#q2",
    "href": "session03/assignment-r.html#q2",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "Draw a barplot of gender whose \\(y\\)-axis represents the percentage of each group.\n\n\ndownload_festival$gender &lt;- factor(download_festival$gender, levels = c(1,2,3), labels = c('Male', 'Female', 'Non-binary'))\n\nggplot(download_festival, aes(x=gender)) + \n  geom_bar() + \n  xlab('Gender') + \n  ylab('Percentage') + \n  scale_y_continuous(labels = scales::percent) + \n  ggtitle('Percentage of Gender')",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#q3",
    "href": "session03/assignment-r.html#q3",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "Draw a histogram of day_1. Is there anything wrong with this variable?\n\n\nprint('Draw a histogram of “day_1”. Is there anything wrong with this variable?')\n\n[1] \"Draw a histogram of “day_1”. Is there anything wrong with this variable?\"\n\nggplot(download_festival, aes(x=day_1)) + \n  geom_histogram(color = 1, bins=15) + \n  xlab('Day 1') + \n  ylab('Frequency / Count') + \n  ggtitle('Histogram of hygiene score in the 1st Day (Raw data)')\n\n\n\n\n\n\n\n  print('An outlier found.')\n\n[1] \"An outlier found.\"",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#q4",
    "href": "session03/assignment-r.html#q4",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "How many cases have missing values for day_2?\n\n\nprint('How many cases have missing values for “day_2”?')\n\n[1] \"How many cases have missing values for “day_2”?\"\n\nsummary(download_festival)\n\n   ticket_no           gender        day_1            day_2       \n Min.   :2111   Male      :277   Min.   : 0.020   Min.   :0.0000  \n 1st Qu.:3096   Female    :443   1st Qu.: 1.312   1st Qu.:0.4100  \n Median :3620   Non-binary: 90   Median : 1.790   Median :0.7900  \n Mean   :3616                    Mean   : 1.793   Mean   :0.9609  \n 3rd Qu.:4155                    3rd Qu.: 2.230   3rd Qu.:1.3500  \n Max.   :4765                    Max.   :20.020   Max.   :3.4400  \n                                                  NA's   :546     \n     day_3       \n Min.   :0.0200  \n 1st Qu.:0.4400  \n Median :0.7600  \n Mean   :0.9765  \n 3rd Qu.:1.5250  \n Max.   :3.4100  \n NA's   :687     \n\nprint('Answer\\'s on the 7th row, 546 cases missing.')\n\n[1] \"Answer's on the 7th row, 546 cases missing.\"",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/assignment-r.html#q5",
    "href": "session03/assignment-r.html#q5",
    "title": "Laboratory 01 (feat. R)",
    "section": "",
    "text": "Exclude the outlier.\n\n\nprint('Exclude Ms. Laundry Pod.')\n\n[1] \"Exclude Ms. Laundry Pod.\"\n\ndownload_festival_no_outliers &lt;- download_festival[download_festival$day_1 &lt;= 5, ]\nggplot(download_festival_no_outliers, aes(x=day_1)) + \n  geom_histogram(color = 1, bins=15) + \n  labs(title='Histogram of Hygiene Scores on the 1st Day', \n        subtitle='Outlier removed') + \n  xlab('Day 1') + \n  ylab('Frequency / Count')\n\n\n\n\n\n\n\n\n[EOF]",
    "crumbs": [
      "Labs",
      "Session03",
      "Laboratory 01 (feat. R)"
    ]
  },
  {
    "objectID": "session03/lecture-notes.html",
    "href": "session03/lecture-notes.html",
    "title": "Session 03: Descriptive Statistics",
    "section": "",
    "text": "preliminary data screening\nchoosing appropriate descriptive statistics based on data distribution\nthoroughly reporting statistical findings, e.g.:\n\ndata cleaning\noutlier handling procedures"
  },
  {
    "objectID": "session03/lecture-notes.html#summary",
    "href": "session03/lecture-notes.html#summary",
    "title": "Session 03: Descriptive Statistics",
    "section": "",
    "text": "preliminary data screening\nchoosing appropriate descriptive statistics based on data distribution\nthoroughly reporting statistical findings, e.g.:\n\ndata cleaning\noutlier handling procedures"
  },
  {
    "objectID": "session03/lecture-notes.html#why-doing-descriptive-statistics",
    "href": "session03/lecture-notes.html#why-doing-descriptive-statistics",
    "title": "Session 03: Descriptive Statistics",
    "section": "Why doing descriptive statistics?",
    "text": "Why doing descriptive statistics?\nScreening:\n- missing data - unusually values - too small groups - mistakes - e.g. impossible values\nThen: correct them or drop them"
  },
  {
    "objectID": "session03/lecture-notes.html#frequency-distributions",
    "href": "session03/lecture-notes.html#frequency-distributions",
    "title": "Session 03: Descriptive Statistics",
    "section": "Frequency Distributions",
    "text": "Frequency Distributions\n\nUsed for preliminary data screening\nFrequency tables for categorical variables\nHistograms for continuous variables\n\n\nimport pandas as pd\nfrom scipy import stats\nimport seaborn as sns\n\n\ntemphr10 = pd.read_spss('./datasets/temphr10.sav')\ntemphr10.columns\n\nIndex(['sex', 'hr', 'temp_Fahrenheit', 'temp_Celcius', 'Likert_rating'], dtype='object')\n\n\n\n# Frequency Tables for Categorical Variables (slides p.6)\ntemphr10['sex'].value_counts()\n\nsex\nMale      7\nFemale    3\nName: count, dtype: int64\n\n\n\n# Filtering missing data with haircolor dataset (p. 6)\nhaircolor = pd.read_spss('./datasets/haircolor.sav')\nhaircolor.columns\n\nIndex(['casenumber', 'haircolor', 'newgrouphaircolor'], dtype='object')\n\n\n\n# To count missing value, use sum() and isnull(). \nhaircolor.isnull().sum()\n\ncasenumber           0\nhaircolor            4\nnewgrouphaircolor    5\ndtype: int64\n\n\n\n\n\ncasenumber           0\nhaircolor            4\nnewgrouphaircolor    5\ndtype: int64\n\n\n\nThe total \\(N\\) for the study was 50. The sample included these hair color groups: 32% brown, 24% black, 16% blond, 6% red, 6% gray, 4% pink, 2% blue, 2% out‐of‐range scores, and 8% missing values."
  },
  {
    "objectID": "session03/lecture-notes.html#central-tendency-measures",
    "href": "session03/lecture-notes.html#central-tendency-measures",
    "title": "Session 03: Descriptive Statistics",
    "section": "Central Tendency Measures",
    "text": "Central Tendency Measures\n\nMode: Most frequent value\nMedian: Middle value\nMean: \\[M = \\bar{X} = \\frac{\\sum_{i=1}^N X_i}{N}\\]"
  },
  {
    "objectID": "session03/lecture-notes.html#variabilitydispersion-measures",
    "href": "session03/lecture-notes.html#variabilitydispersion-measures",
    "title": "Session 03: Descriptive Statistics",
    "section": "Variability/Dispersion Measures",
    "text": "Variability/Dispersion Measures\n\nRange: Max - Min\nInterquartile Range (IQR): Q3 - Q1\nVariance: \\[s^2 = \\frac{\\sum_{i=1}^N (X_i - M)^2}{N - 1}\\]\nStandard Deviation: \\[s = \\sqrt{s^2}\\]\n\n\n# BMI dataset and the boxplot\nbmi = pd.read_spss('./datasets/bmi.sav')\nbmi.columns\n\nIndex(['idnumber', 'Sex', 'Weight', 'height', 'BMI'], dtype='object')\n\n\n\nsns.boxplot(data=bmi, x='Sex', y='BMI')"
  },
  {
    "objectID": "session03/lecture-notes.html#z-scores",
    "href": "session03/lecture-notes.html#z-scores",
    "title": "Session 03: Descriptive Statistics",
    "section": "Z-scores",
    "text": "Z-scores\n\\[z = \\frac{X - M}{SD}\\]"
  },
  {
    "objectID": "session03/lecture-notes.html#normal-distribution",
    "href": "session03/lecture-notes.html#normal-distribution",
    "title": "Session 03: Descriptive Statistics",
    "section": "5. Normal Distribution",
    "text": "5. Normal Distribution\n\nBell-shaped curve\n68-95-99.7 rule for standard deviations"
  },
  {
    "objectID": "session03/lecture-notes.html#skewness-and-kurtosis",
    "href": "session03/lecture-notes.html#skewness-and-kurtosis",
    "title": "Session 03: Descriptive Statistics",
    "section": "6. Skewness and Kurtosis",
    "text": "6. Skewness and Kurtosis\n\nMeasures of distribution shape\nSkewness: \\[\\frac{1}{N} \\sum(\\frac{X - M}{SD})^3\\]\nKurtosis: \\[\\frac{1}{N} \\sum(\\frac{X - M}{SD})^4\\]"
  },
  {
    "objectID": "session03/lecture-notes.html#assessing-normality",
    "href": "session03/lecture-notes.html#assessing-normality",
    "title": "Session 03: Descriptive Statistics",
    "section": "7. Assessing Normality",
    "text": "7. Assessing Normality\n\nHistograms with normal curve\nQ-Q plots"
  },
  {
    "objectID": "session03/lecture-notes.html#outliers",
    "href": "session03/lecture-notes.html#outliers",
    "title": "Session 03: Descriptive Statistics",
    "section": "8. Outliers",
    "text": "8. Outliers\n\nOften defined as |z| &gt; 3.29"
  },
  {
    "objectID": "session03/lecture-notes.html#reporting-descriptive-statistics",
    "href": "session03/lecture-notes.html#reporting-descriptive-statistics",
    "title": "Session 03: Descriptive Statistics",
    "section": "9. Reporting Descriptive Statistics",
    "text": "9. Reporting Descriptive Statistics\n\nInclude sample size, measures of central tendency, and dispersion\nDescribe distribution shape and presence of outliers\nUse appropriate visualizations (histograms, boxplots)"
  },
  {
    "objectID": "session04/assignment.html",
    "href": "session04/assignment.html",
    "title": "Laboratory 02",
    "section": "",
    "text": "A researcher wondered whether a fish or cat made a better pet. He found some people who had either fish or cats as pets and measured their life satisfaction and how much they like animals. The data are saved in pets.sav.",
    "crumbs": [
      "Labs",
      "Session04",
      "Laboratory 02"
    ]
  },
  {
    "objectID": "session04/assignment.html#q1-frequency-distribution-of-pet",
    "href": "session04/assignment.html#q1-frequency-distribution-of-pet",
    "title": "Laboratory 02",
    "section": "Q1: Frequency distribution of pet",
    "text": "Q1: Frequency distribution of pet\nIn SPSS, the Frequency function in Descriptive Statistics provides Frequency table with bar chart by default. I’ll follow this flavor in my report.\nAnswer\n\n\n\nPet\nFrequency\nPercentage\n\n\n\n\nCat\n8\n60%\n\n\nFish\n12\n40%\n\n\nTotal\n20\n100%\n\n\n\nThe total \\(N = 20\\) includes 60% of cat owners and 40% of fish owners. No missing value.\nFor the bar chart, see Figure 1\nSolution\n\n# I use function for keeping the global namespace clean. \ndef freq_distb(df, col_name):\n    column = df[str(col_name)]\n    print('1. Frequency: ')\n    freq = column.value_counts()\n    print(freq)\n    print('----\\n2. Percentage(%): ')\n    percentage = column.value_counts(normalize=True) * 100\n    print(percentage)\n\nfreq_distb(pets, col_name='pet')\n\n1. Frequency: \npet\nFish    12\nCat      8\nName: count, dtype: int64\n----\n2. Percentage(%): \npet\nFish    60.0\nCat     40.0\nName: proportion, dtype: float64\n\n\n\n# Plotting frequency\nsns.countplot(data=pets, x='pet')\n\n\n\n\n\n\n\nFigure 1: Bar Chart on Frequency of pet",
    "crumbs": [
      "Labs",
      "Session04",
      "Laboratory 02"
    ]
  },
  {
    "objectID": "session04/assignment.html#q2-life-satisfaction-of-pet-owners",
    "href": "session04/assignment.html#q2-life-satisfaction-of-pet-owners",
    "title": "Laboratory 02",
    "section": "Q2: Life satisfaction of pet owners",
    "text": "Q2: Life satisfaction of pet owners\nAnswer\nBased on the box plot (Figure 2), life satisfaction of cat and fish owners differ in aspects below:\n\nMedian (\\(Q_2\\)): The box-plot shows that cat people have higher median of life satisfaction than the fish people.\nIQR (\\(Q_3 - Q_1\\)): The IQR for cat people is more concentrated, while for fish people, their IQR is much wider, which indicates more variability in life satisfaction.\nRange: Cat people’s life satisfaction ranges from around 50 to 70, with an outlier below 50. Fish owners’ life satisfaction ranges from about 15 to 65, with an outlier below 15.\n\nIn conclusion, cat owners generally report higher and more consistent life satisfaction compared to fish owners.\nCats win! ฅ•ﻌ•ฅ\nSolution\n\n# Plotting Box-plot for Cat and Fish.\nsns.boxplot(pets, x='pet', y='life_satisfaction', hue='pet')\nplt.show()\n\n\n\n\n\n\n\nFigure 2: Life satisfaction",
    "crumbs": [
      "Labs",
      "Session04",
      "Laboratory 02"
    ]
  },
  {
    "objectID": "session04/assignment.html#q3q5-histogram-of-life_satisfaction-by-pet",
    "href": "session04/assignment.html#q3q5-histogram-of-life_satisfaction-by-pet",
    "title": "Laboratory 02",
    "section": "Q3+Q5: Histogram of life_satisfaction (+ by pet)",
    "text": "Q3+Q5: Histogram of life_satisfaction (+ by pet)\nAnswer (for Question 3): The Overall Distribution (Figure 3)\n\nThe overall distribution shows that the highest concentration of life satisfaction is around 50.\nExtremely low life satisfaction scores exist, with individuals reporting very low scores (around 10 to 30).\nThe peak at around 50 reflects the central tendency for both groups, but, again, the tail on the left side of this figure suggests significantly lower satisfaction exists.\n\nSolution\n\nsns.histplot(data=pets, x='life_satisfaction', bins=15)\nplt.show\n\n\n\n\n\n\n\nFigure 3: Overall distribution of life satisfaction\n\n\n\n\n\nAnswer (for Question 5): A side-by-side comparison (Figure 4)\n\nFor cat owners:\n\nLife satisfaction scores are concentrated between 50 and 70, with most scores around 60.\nThe distribution is tight, with one outlier who has a significantly lower life satisfaction score.\n\nFor fish owners:\n\nLife satisfaction scores are more spreading, from around 10 to 60.\nThe most frequent score is around 40-50, with several people having low satisfaction (below 20).\nCompared to cat owners, fish owners have a wider range of life satisfaction scores.\n\n\n\ng = sns.FacetGrid(data=pets, col='pet', hue='pet')\ng = g.map(plt.hist, 'life_satisfaction', bins=15)\nplt.show()\n\n\n\n\n\n\n\nFigure 4: Overall distribution of life satisfaction",
    "crumbs": [
      "Labs",
      "Session04",
      "Laboratory 02"
    ]
  },
  {
    "objectID": "session04/assignment.html#q4-measurements-to-describe-variables",
    "href": "session04/assignment.html#q4-measurements-to-describe-variables",
    "title": "Laboratory 02",
    "section": "Q4: Measurements to describe variables",
    "text": "Q4: Measurements to describe variables\nAnswer\nBased on the dataset and output:\n\n\n\nValues\nOverall (pet)\nCat\nFish\n\n\n\n\nMean\n46.95\n60.13\n38.17\n\n\nMedian\n47.50\n63.00\n44.00\n\n\nStandard Deviation\n17.51\n11.10\n15.51\n\n\nRange\n66.00\n35.00\n50.00\n\n\nInter-quartile range\n19.50\n8.50\n15.75\n\n\n\n\nFor central tendency: Median (\\(Q_2\\))\n\nMedian is a robust measure of central tendency, especially when there is a wide range of values and potential outliers. It is less affected by extreme values than the mean: providing a better representation of the typical life satisfaction for both cat and fish owners.\n\nFor dispersion: IQR (\\(Q_3 - Q_1\\))\n\nIQR measures the spread of the middle 50% of the data, which is less influenced by outliers compared to the full range or standard deviation.\n\n\nSolution\n\ndef get_stats(group, group_name, column):\n    dict_stats = {\n        'Name': group_name,\n        'Mean': group[column].mean(),\n        'Median': group[column].median(),\n        'SD': group[column].std(),\n        'Range': group[column].max() - group[column].min(),\n        'IQR': stats.iqr(group[column])\n    }\n    return dict_stats\n\n\ndef describe_var():\n    # Overall\n    overall_stats = get_stats(pets, \n                              'Overall', \n                              'life_satisfaction')\n    # Meow\n    cat_stats = get_stats(pets[pets['pet'] == 'Cat'], \n                          'Cat', \n                          'life_satisfaction')\n    # Fish\n    fish_stats = get_stats(pets[pets['pet'] == 'Fish'], \n                           'Fish', \n                           'life_satisfaction')\n    # Output as a pd.DataFrame\n    df = pd.DataFrame([overall_stats, cat_stats, fish_stats])\n    return df\n\nprint(describe_var())\n\n      Name       Mean  Median         SD  Range    IQR\n0  Overall  46.950000    47.5  17.506315   66.0  19.50\n1      Cat  60.125000    63.0  11.102606   35.0   8.50\n2     Fish  38.166667    44.0  15.508551   50.0  15.75",
    "crumbs": [
      "Labs",
      "Session04",
      "Laboratory 02"
    ]
  },
  {
    "objectID": "session04/assignment.html#q5-histogram-by-pet",
    "href": "session04/assignment.html#q5-histogram-by-pet",
    "title": "Laboratory 02",
    "section": "Q5: Histogram by pet",
    "text": "Q5: Histogram by pet\nSee Question 3 + Question 5 (in Section 3.3).",
    "crumbs": [
      "Labs",
      "Session04",
      "Laboratory 02"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#footnotes",
    "href": "session16-17/assignment.html#footnotes",
    "title": "Laboratory 09",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think Andy once said in the book that we should always do the correction for sphericity.↩︎",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session16-17/assignment.html#solutions-1",
    "href": "session16-17/assignment.html#solutions-1",
    "title": "Laboratory 09",
    "section": "Solutions",
    "text": "Solutions\n\nQ4-Q6: Repeated-measures ANOVA\nSince Mauchly’s sphericity test did not indicate a significant violation of the sphericity test (\\(p = 0.190\\), just as what you planned), sphericity assumption assumed. The repeated-measures ANOVA was conducted and report as follows:\n\n\n\nTable 2: Repeated-measure ANOVA of alcohol dose on attractiveness ratings\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\\(\\eta_p^2\\)\n\n\n\n\nDoseBright\n7434.192\n3\n2478.064\n25.198\n&lt;.001\n.502\n\n\nError\n7375.808\n75\n98.344\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Dose of alcohol on attractiveness ratings in bright lighting conditions\n\n\n\nThe effect of alcohol dose on mean attractiveness ratings in bright lighting is significant, \\(F(3, 75) = 25.198\\), \\(p &lt; 0.001\\), \\(\\eta_p^2 = 0.502\\), indicating a large effect size, suggesting that the dose of alcohol explains about 50.2% of the variance in attractiveness ratings. The chart shown in Figure 2 also shows a decreasing trend in attractiveness ratings as the alcohol dose increases from 0 to 6 pints. This trend suggests that higher doses of alcohol may lead to lower attractiveness ratings under bright lighting conditions, contrary to the beer-goggles effect hypothesis. These results indicate that, in bright lighting, increasing alcohol consumption tends to decrease attractiveness ratings, showing a clear dose-response relationship.\n\n\nQ7: The magnification of the beer goggles effect\nAgain, Mauchly’s test did not indicate a significant violation of the sphericity for both variables (\\(p_{D} = 0.454\\), \\(p_{D \\times L} = 0.768\\)), which may seldom happen to the real world data 1. Anyway, the repeated-measures ANOVA was conducted and report as follows:\n\n\n\nTable 3: Repeated-measure ANOVA of alcohol dose and lighting interact to the attractiveness ratings\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\\(\\eta_p^2\\)\n\n\n\n\nDOSE\n38591.654\n3\n12863.885\n104.385\n&lt;.001\n.807\n\n\nError (\\(D\\))\n9242.596\n75\n123.235\n\n\n\n\n\nLIGHTING\n1993.923\n1\n1993.923\n23.421\n&lt;.001\n.484\n\n\nError (\\(L\\))\n2128.327\n25\n85.133\n\n\n\n\n\n\\(D \\times L\\)\n5765.423\n3\n1921.808\n22.218\n&lt;.001\n.471\n\n\nError (\\(D \\times L\\))\n6487.327\n75\n86.498\n\n\n\n\n\n\n\n\n\nThere is a significant main effect of alcohol dose on attractiveness ratings, \\(F_D(3, 75) = 104.385\\), \\(p &lt; 0.001\\), with a large effect size (\\(\\eta_p^2 = 0.807\\)). This indicates that different alcohol doses significantly change attractiveness ratings. A significant main effect of lighting on attractiveness ratings is also observed, \\(F_L(1, 25) = 23.421\\), \\(p &lt; 0.001\\), with a substantial effect size (\\(\\eta_p^2 = 0.484\\)). This underscores the impact of lighting conditions on perceived attractiveness.\nThe interaction between alcohol dose and lighting is significant, \\(F_{D \\times L}(3, 75) = 22.218\\), \\(p &lt; 0.001\\), with \\(\\eta_p^2 = 0.471\\). This suggests that the effect of alcohol dose on attractiveness ratings depends on the lighting condition, indicating that lighting may amplify the beer goggles effect.\n\n\n\n\n\n\nFigure 3: Dose of alcohol and lighting conditions on attractiveness ratings. For the lighting condition, level 1 for dim, level 2 for bright.\n\n\n\nOverall, as Table 3 and Figure 3 suggested, the significant interaction effect indicates that both alcohol dose and lighting conditions jointly influence attractiveness ratings. The beer goggles effect is more pronounced under certain lighting conditions, with the potential for lighting to either enhance or mitigate the influence of alcohol on attractiveness ratings. This complex interaction shows the importance of considering environmental factors like lighting when evaluating social behaviors influenced by alcohol.",
    "crumbs": [
      "Labs",
      "Session16 17",
      "Laboratory 09"
    ]
  },
  {
    "objectID": "session17-18/assignment.html",
    "href": "session17-18/assignment.html",
    "title": "Laboratory 10",
    "section": "",
    "text": "Although conducting ANOVA or ANCOVA in the world of Python or R is not as straightforward as training a even more complex ML model. Plotting could be easier. So,\nRecall back our second question:\n\nDraw a line chart using the ⟨ Plots ⟩ option. Let ⟨ Horizontal Axis ⟩ and ⟨ Separate Lines ⟩ be alcohol consumption and facial attractiveness respectively, and include error bars showing the 95% confidence intervals of the means.\n\nIt’s quite simple to just draw a line-plot with error bar.",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#the-line-plot-and-ggplot2",
    "href": "session17-18/assignment.html#the-line-plot-and-ggplot2",
    "title": "Laboratory 10",
    "section": "",
    "text": "Although conducting ANOVA or ANCOVA in the world of Python or R is not as straightforward as training a even more complex ML model. Plotting could be easier. So,\nRecall back our second question:\n\nDraw a line chart using the ⟨ Plots ⟩ option. Let ⟨ Horizontal Axis ⟩ and ⟨ Separate Lines ⟩ be alcohol consumption and facial attractiveness respectively, and include error bars showing the 95% confidence intervals of the means.\n\nIt’s quite simple to just draw a line-plot with error bar.",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#q1-the-scatterplot",
    "href": "session17-18/assignment.html#q1-the-scatterplot",
    "title": "Laboratory 10",
    "section": "Q1: The scatterplot",
    "text": "Q1: The scatterplot\nThe scatterplot between how drunk the person was before the treatment (\\(X_c\\)) and how well they felt after treatment (\\(Y\\)) is shown as Figure 1, \\(X_c\\) and \\(Y\\) showed a linear relation with no bivariate outliers.\n\n\n\n\n\n\nFigure 1: The outcome \\(Y\\) against the covariate \\(X_c\\).",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#q2-independence-of-x_c-and-a",
    "href": "session17-18/assignment.html#q2-independence-of-x_c-and-a",
    "title": "Laboratory 10",
    "section": "Q2: Independence of \\(X_c\\) and \\(A\\)",
    "text": "Q2: Independence of \\(X_c\\) and \\(A\\)\nTo reveal whether people in the three groups differ in how drunk they were before the treatment, a one-way ANOVA was conducted and the results were listed as Table 1.\n\n\n\nTable 1\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\n\n\n\ndrink\n8.400\n2\n4.200\n1.355\n.295\n\n\nError\n37.200\n12\n3.100\n\n\n\n\n\n\n\n\nThe result showed that the main effect of drink is not significant, with \\(F(2, 12) = 1.355\\), \\(p=0.295\\), that indicated that statically there were no differences on the average drunk level between all three groups. That means the \\(X_c\\) and \\(A\\) are independent, the assumption of ANCOVA appeared to be satisfied.",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#q3-homogeneity-of-regression-slopes",
    "href": "session17-18/assignment.html#q3-homogeneity-of-regression-slopes",
    "title": "Laboratory 10",
    "section": "Q3: Homogeneity of regression slopes",
    "text": "Q3: Homogeneity of regression slopes\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\n\n\n\ndrink * drunk\n11.610\n3\n3.870\n6.951\n0.007\n\n\nError\n6.124\n11\n0.557\n\n\n\n\n\nTo assess whether there was an interaction between \\(A\\) and \\(X_c\\), a preliminary ANCOVA was run using the SPSS GLM procedure with a custom model that included an \\(A \\times X_c\\) interaction term. This interaction was statistically significant with an \\(F(3, 11) = 6.951\\), \\(p = 0.007\\). This indicates that the effect of the treatment on how well the person feels depends on how drunk they felt before the treatment.\nIn conclusion, there is a significant interaction between the treatment and the covariate, meaning the initial feeling of drunkenness influences the treatment effect on how well the person feels.3 The assumption of homogeneity of regression slopes may be violated.",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#q4-ancova-and-the-adjusted-means-of-group-a_1-a_2-and-a_3.",
    "href": "session17-18/assignment.html#q4-ancova-and-the-adjusted-means-of-group-a_1-a_2-and-a_3.",
    "title": "Laboratory 10",
    "section": "Q4: ANCOVA and the adjusted means of Group \\(A_1\\), \\(A_2\\) and \\(A_3\\).",
    "text": "Q4: ANCOVA and the adjusted means of Group \\(A_1\\), \\(A_2\\) and \\(A_3\\).\n\n\n\n\n\n\n\n\n\n\n\n\nSource\nSS\n\\(df\\)\nMS\n\\(F\\)\n\\(p\\)\n\\(\\eta_p^2\\)\n\n\n\n\ndrunk\n9.856\n1\n9.856\n24.568\n&lt;0.001\n0.691\n\n\ndrink\n3.464\n2\n1.732\n4.318\n0.041\n0.440\n\n\nError\n4.413\n11\n0.401\n\n\n\n\n\n\nAn ANCOVA was conducted to examine the effect of drink type on recovery while controlling for initial drunkenness. The effect of how drunk the person felt the night before was significant, \\(F(1,11) = 24.568\\), \\(p &lt; 0.001\\), \\(\\eta_p^2 = 0.691\\), indicated that pre-existing drunkenness significantly affects how well the person feels. The treatment had a significant effect on how well the person felt, \\(F(2,11) = 4.318\\), \\(p = 0.041\\), \\(\\eta_p^2 = 0.440\\).\nThe adjusted means were given in Table 2, suggested that participants who drank Lucozade felt better than those who drank Water or Cola.\n\n\n\nTable 2: Adjusted means (evaluated at initial drunkenness = 3.40).\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean of \\(Y\\), adjusted for \\(X_c\\)\nUnadjusted Mean of \\(Y\\)\n\n\n\n\nGroup 1\nWater\n5.110\n5.00\n\n\nGroup 2\nLucozade\n6.239\n5.80\n\n\nGroup 3\nCola\n5.252\n5.80",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#q5-anova-or-ancova",
    "href": "session17-18/assignment.html#q5-anova-or-ancova",
    "title": "Laboratory 10",
    "section": "Q5: ANOVA or ANCOVA",
    "text": "Q5: ANOVA or ANCOVA\nIf there’s another other chance for me I would like to say I prefer neither of them then go for casual inference. I swear I WILL master this method after Christmas and Lunar New Year recess.\nIn general, ANOVA is used to compare the means of different groups and determine if there are statistically significant differences between them. It assumes that all groups are treated equally and does not adjust for any confounding variables or covariates that might affect the dependent variable. In contrast, ANCOVA extends ANOVA by including one or more covariate(s) that can influence the dependent variable. By controlling for these covariates, ANCOVA adjusts the group means, providing a clearer understanding of the treatment effects while accounting for variability attributed to other factors.\nFrom we see significant effects both from the covariate (“drunk”) and the treatment (“drink”), indicating that initial drunkenness plays a substantial role in how participants feel subsequently.\nANCOVA is the better choice here for several reasons:\n\nPower: By accounting for the variance explained by initial drunkenness, ANCOVA reduces error variance and increases statistical power to detect treatment effects. This is particularly important with the small sample size (\\(n = 15\\)).\nPrecision: Initial drunkenness strongly predicts recovery (shown by the large effect size). Controlling for this relationship provides a more precise estimate of the treatment effect.\nIndividual Differences: Even with successful randomization, individuals entered the study with different levels of drunkenness. ANCOVA adjusts for these pre-existing differences, giving us a clearer picture of the drink effects.\n\nTherefore, while both analyses are valid given the successful randomization, ANCOVA provides a more powerful and precise test of the causal relationship between drink type and recovery in this study.",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#q6-everything-wrapped-up-the-summary",
    "href": "session17-18/assignment.html#q6-everything-wrapped-up-the-summary",
    "title": "Laboratory 10",
    "section": "Q6: Everything wrapped up (the summary)",
    "text": "Q6: Everything wrapped up (the summary)\nI think the",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  },
  {
    "objectID": "session17-18/assignment.html#footnotes",
    "href": "session17-18/assignment.html#footnotes",
    "title": "Laboratory 10",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnd it is now owned by Japanese (SUNTORY bought it from GSK in 2013).↩︎\nThe leading brand? That should be the reeeeeeeeeeeeeed one.↩︎\nI tested on myself and that’s true.↩︎",
    "crumbs": [
      "Labs",
      "Session17 18",
      "Laboratory 10"
    ]
  }
]